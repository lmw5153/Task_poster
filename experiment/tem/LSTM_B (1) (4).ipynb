{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526bc5-cd1a-4fac-a796-57902266c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636876fb-747b-4480-8ea0-8ff0618bd573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 22:52:58.929496: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 22:52:59.034490: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-31 22:52:59.034520: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-31 22:52:59.513903: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-31 22:52:59.513968: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-31 22:52:59.513977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3850e1af-7ef4-47f2-b130-6732c47014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = 'tem'\n",
    "target_X= pd.read_csv(f\"../data/{data}_train_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "target_y =pd.read_csv(f\"../data/{data}_train_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "X_train = target_X[:-round(target_X.shape[0]*0.2),:].astype(np.float32)\n",
    "y_train = target_y[:-round(target_y.shape[0]*0.2)].astype(np.float32)\n",
    "\n",
    "X_train_val= target_X[-round(target_X.shape[0]*0.2):,:].astype(np.float32)\n",
    "y_train_val =target_y[-round(target_y.shape[0]*0.2):].astype(np.float32)\n",
    "\n",
    "\n",
    "test_X= pd.read_csv(f\"../data/{data}_val_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "test_y =pd.read_csv(f\"../data/{data}_val_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "#y_train.astype(np.float32)\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3aff672-3e0d-4a08-9d95-0a0eacbdbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],1,y_train.shape[1]\n",
    "\n",
    "#################################################################################\n",
    "# nbeats 모델 생성 함수\n",
    "def build_model(input_timesteps,features,output_timesteps):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(input_timesteps, features)))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    #model.add(LSTM(unit, return_sequences=True))\n",
    "    # Use Lambda layer to select the last 'output_timesteps' outputs\n",
    "    model.add(Lambda(lambda x: x[:, -24:, :]))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "#################################################################################\n",
    "# 부트스트랩 샘플링\n",
    "# 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    input_timesteps,features,output_timesteps= hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = build_model(input_timesteps,features,output_timesteps)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                validation_data = [X_train_val,y_train_val])\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce9eb03-7500-4d76-af24-72f76cc67df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 22:53:06.615084: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-10-31 22:53:06.615118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-10-31 22:53:06.615532: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 1.8682 - val_loss: 1.2860\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 1.1880 - val_loss: 0.8677\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.8588 - val_loss: 0.6517\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6768 - val_loss: 0.5371\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5782 - val_loss: 0.5007\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5329 - val_loss: 0.4961\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5132 - val_loss: 0.4901\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5061 - val_loss: 0.4573\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4928 - val_loss: 0.4952\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4891 - val_loss: 0.4527\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4816 - val_loss: 0.4597\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4766 - val_loss: 0.4584\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4787 - val_loss: 0.4932\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4755 - val_loss: 0.4668\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4672 - val_loss: 0.4493\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4605 - val_loss: 0.4648\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4686 - val_loss: 0.4425\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4539 - val_loss: 0.4776\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4664 - val_loss: 0.4426\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4661 - val_loss: 0.4823\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4609 - val_loss: 0.4663\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4565 - val_loss: 0.4474\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4516 - val_loss: 0.4391\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4474 - val_loss: 0.4452\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4379 - val_loss: 0.4419\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4346 - val_loss: 0.4383\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4398 - val_loss: 0.4359\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4396 - val_loss: 0.4371\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4326 - val_loss: 0.4642\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4350 - val_loss: 0.4286\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4256 - val_loss: 0.4365\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4340 - val_loss: 0.4685\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4354 - val_loss: 0.4477\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4279 - val_loss: 0.4489\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4265 - val_loss: 0.4709\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4208 - val_loss: 0.4825\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4152 - val_loss: 0.4925\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4219 - val_loss: 0.4656\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4143 - val_loss: 0.4393\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4095Restoring model weights from the end of the best epoch: 30.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4095 - val_loss: 0.4634\n",
      "Epoch 40: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 7s 77ms/step - loss: 1.7423 - val_loss: 1.1591\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 1.0750 - val_loss: 0.7885\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.8066 - val_loss: 0.6520\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.6528 - val_loss: 0.5301\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.5683 - val_loss: 0.4997\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.5271 - val_loss: 0.5049\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.5085 - val_loss: 0.4876\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4951 - val_loss: 0.4558\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4994 - val_loss: 0.4905\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4890 - val_loss: 0.4600\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4784 - val_loss: 0.4595\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4703 - val_loss: 0.4569\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4778 - val_loss: 0.4537\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4710 - val_loss: 0.4786\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4770 - val_loss: 0.4488\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4671 - val_loss: 0.4521\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4641 - val_loss: 0.4697\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4578 - val_loss: 0.4501\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4680 - val_loss: 0.4997\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4481 - val_loss: 0.4605\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4575 - val_loss: 0.4455\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4549 - val_loss: 0.4453\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4548 - val_loss: 0.4555\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4493 - val_loss: 0.4419\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4429 - val_loss: 0.4660\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4569 - val_loss: 0.4562\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4503 - val_loss: 0.4444\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4441 - val_loss: 0.4421\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4378 - val_loss: 0.4405\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4277 - val_loss: 0.4404\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4239 - val_loss: 0.4475\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4318 - val_loss: 0.4600\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4206 - val_loss: 0.4468\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4142 - val_loss: 0.4632\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4234 - val_loss: 0.4675\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4299 - val_loss: 0.4632\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4099 - val_loss: 0.4551\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4110 - val_loss: 0.4625\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4246 - val_loss: 0.4552\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4060Restoring model weights from the end of the best epoch: 30.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4060 - val_loss: 0.4607\n",
      "Epoch 40: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 103ms/step - loss: 1.7752 - val_loss: 1.1787\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 1.1087 - val_loss: 0.8320\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.8173 - val_loss: 0.6414\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.6576 - val_loss: 0.5283\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.5715 - val_loss: 0.4834\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.5228 - val_loss: 0.5068\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.5224 - val_loss: 0.4680\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4943 - val_loss: 0.4588\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4888 - val_loss: 0.4609\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4859 - val_loss: 0.4577\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4822 - val_loss: 0.4849\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4859 - val_loss: 0.4713\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4845 - val_loss: 0.4625\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4690 - val_loss: 0.4459\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4678 - val_loss: 0.4442\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4638 - val_loss: 0.4361\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4632 - val_loss: 0.4890\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4698 - val_loss: 0.4414\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4602 - val_loss: 0.4533\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4674 - val_loss: 0.4702\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4534 - val_loss: 0.4825\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4601 - val_loss: 0.4661\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4490 - val_loss: 0.4957\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4666 - val_loss: 0.4486\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4489 - val_loss: 0.4411\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4436Restoring model weights from the end of the best epoch: 16.\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4436 - val_loss: 0.4385\n",
      "Epoch 26: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 111ms/step - loss: 1.6546 - val_loss: 1.0470\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.9815 - val_loss: 0.7365\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.7395 - val_loss: 0.5759\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.5928 - val_loss: 0.5308\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.5483 - val_loss: 0.4858\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.5241 - val_loss: 0.4704\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5013 - val_loss: 0.4791\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4989 - val_loss: 0.4673\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4895 - val_loss: 0.4887\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4772 - val_loss: 0.4665\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4815 - val_loss: 0.4506\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4753 - val_loss: 0.4502\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4733 - val_loss: 0.4714\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4711 - val_loss: 0.4647\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4646 - val_loss: 0.4637\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4726 - val_loss: 0.4434\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4661 - val_loss: 0.4554\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4608 - val_loss: 0.4460\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4706 - val_loss: 0.4646\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4590 - val_loss: 0.4724\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4573 - val_loss: 0.4948\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4549 - val_loss: 0.4799\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4583 - val_loss: 0.4692\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4534 - val_loss: 0.4501\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4554 - val_loss: 0.4360\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4511 - val_loss: 0.4678\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4512 - val_loss: 0.4563\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4487 - val_loss: 0.4482\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4448 - val_loss: 0.4577\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4517 - val_loss: 0.4536\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4625 - val_loss: 0.4283\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4497 - val_loss: 0.4478\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4302 - val_loss: 0.4509\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4354 - val_loss: 0.4910\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4401 - val_loss: 0.4470\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4241 - val_loss: 0.4410\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4250 - val_loss: 0.4385\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4264 - val_loss: 0.4706\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4215 - val_loss: 0.4667\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4194 - val_loss: 0.4790\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4145Restoring model weights from the end of the best epoch: 31.\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4145 - val_loss: 0.4531\n",
      "Epoch 41: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 113ms/step - loss: 1.8553 - val_loss: 1.2459\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 1.1575 - val_loss: 0.8544\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.8498 - val_loss: 0.6546\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6661 - val_loss: 0.5546\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5739 - val_loss: 0.5052\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5318 - val_loss: 0.4749\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5086 - val_loss: 0.4644\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4946 - val_loss: 0.5095\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4986 - val_loss: 0.4671\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4899 - val_loss: 0.4418\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4846 - val_loss: 0.4509\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4726 - val_loss: 0.4664\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4844 - val_loss: 0.4639\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4769 - val_loss: 0.4630\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4791 - val_loss: 0.4405\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4646 - val_loss: 0.4543\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4613 - val_loss: 0.4521\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4603 - val_loss: 0.4555\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.4498 - val_loss: 0.4324\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4506 - val_loss: 0.4471\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4510 - val_loss: 0.4535\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4524 - val_loss: 0.4625\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4479 - val_loss: 0.4545\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4407 - val_loss: 0.4439\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4438 - val_loss: 0.4381\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4370 - val_loss: 0.4542\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4412 - val_loss: 0.4457\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4392 - val_loss: 0.4334\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4437Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4437 - val_loss: 0.4558\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 86ms/step - loss: 1.6356 - val_loss: 1.0863\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 1.0328 - val_loss: 0.7595\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7639 - val_loss: 0.5973\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.6165 - val_loss: 0.5028\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.5524 - val_loss: 0.4983\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5285 - val_loss: 0.4735\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5050 - val_loss: 0.4654\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4940 - val_loss: 0.4523\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4885 - val_loss: 0.4584\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4971 - val_loss: 0.4697\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4821 - val_loss: 0.4804\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4810 - val_loss: 0.4426\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4756 - val_loss: 0.4763\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4818 - val_loss: 0.4512\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4748 - val_loss: 0.4563\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4715 - val_loss: 0.4858\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4705 - val_loss: 0.4374\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4688 - val_loss: 0.4388\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4659 - val_loss: 0.4372\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4647 - val_loss: 0.4638\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4620 - val_loss: 0.4543\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4495 - val_loss: 0.4501\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4516 - val_loss: 0.4605\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4507 - val_loss: 0.4626\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4449 - val_loss: 0.4522\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4433 - val_loss: 0.4384\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4417 - val_loss: 0.4652\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4409 - val_loss: 0.4467\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4346Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4346 - val_loss: 0.4671\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 91ms/step - loss: 1.6623 - val_loss: 1.0906\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 1.0243 - val_loss: 0.7702\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.7559 - val_loss: 0.6020\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.6051 - val_loss: 0.5010\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5368 - val_loss: 0.4758\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5204 - val_loss: 0.4671\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5039 - val_loss: 0.4623\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4942 - val_loss: 0.4765\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4889 - val_loss: 0.4974\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4805 - val_loss: 0.4635\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4754 - val_loss: 0.4886\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4844 - val_loss: 0.4495\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4759 - val_loss: 0.4397\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4645 - val_loss: 0.4724\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4697 - val_loss: 0.4577\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4654 - val_loss: 0.4493\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4606 - val_loss: 0.4526\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4763 - val_loss: 0.4541\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4683 - val_loss: 0.4517\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4646 - val_loss: 0.4404\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4469 - val_loss: 0.4558\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4483 - val_loss: 0.4526\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4499 - val_loss: 0.4314\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4380 - val_loss: 0.4635\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4337 - val_loss: 0.4593\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4376 - val_loss: 0.4532\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4419 - val_loss: 0.4353\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4303 - val_loss: 0.4647\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4348 - val_loss: 0.4668\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4336 - val_loss: 0.4522\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4250 - val_loss: 0.4322\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4268 - val_loss: 0.4461\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4351Restoring model weights from the end of the best epoch: 23.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4351 - val_loss: 0.4642\n",
      "Epoch 33: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 105ms/step - loss: 1.6741 - val_loss: 1.0929\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 1.0153 - val_loss: 0.7632\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7652 - val_loss: 0.5890\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.6267 - val_loss: 0.5212\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.5554 - val_loss: 0.4804\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5250 - val_loss: 0.4812\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.5110 - val_loss: 0.4593\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5023 - val_loss: 0.4902\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4996 - val_loss: 0.4546\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4799 - val_loss: 0.4802\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4778 - val_loss: 0.4509\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4793 - val_loss: 0.4470\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4770 - val_loss: 0.4498\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4708 - val_loss: 0.4730\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4734 - val_loss: 0.4651\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4705 - val_loss: 0.4684\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4640 - val_loss: 0.4561\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4672 - val_loss: 0.4580\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4573 - val_loss: 0.4413\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4535 - val_loss: 0.4363\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4616 - val_loss: 0.4594\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4487 - val_loss: 0.4400\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4604 - val_loss: 0.4387\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4550 - val_loss: 0.4326\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4438 - val_loss: 0.4511\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4415 - val_loss: 0.4591\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4398 - val_loss: 0.4523\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4363 - val_loss: 0.4506\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4432 - val_loss: 0.4408\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4328 - val_loss: 0.4458\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4402 - val_loss: 0.4333\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4336 - val_loss: 0.4407\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4302 - val_loss: 0.4338\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4225Restoring model weights from the end of the best epoch: 24.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4225 - val_loss: 0.4577\n",
      "Epoch 34: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 113ms/step - loss: 1.8080 - val_loss: 1.2622\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 1.1597 - val_loss: 0.8501\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.8384 - val_loss: 0.6437\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.6736 - val_loss: 0.5414\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5736 - val_loss: 0.4936\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5385 - val_loss: 0.5244\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5091 - val_loss: 0.4599\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4996 - val_loss: 0.4667\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4993 - val_loss: 0.4665\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4854 - val_loss: 0.4574\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4815 - val_loss: 0.4800\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4907 - val_loss: 0.4522\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4717 - val_loss: 0.4924\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4717 - val_loss: 0.4524\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4706 - val_loss: 0.4470\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4642 - val_loss: 0.4620\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4690 - val_loss: 0.4572\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4775 - val_loss: 0.4798\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4571 - val_loss: 0.4370\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4575 - val_loss: 0.4542\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4549 - val_loss: 0.4630\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4495 - val_loss: 0.4586\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4498 - val_loss: 0.4394\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4457 - val_loss: 0.4410\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4499 - val_loss: 0.4828\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4466 - val_loss: 0.4330\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4494 - val_loss: 0.4493\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4448 - val_loss: 0.4721\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4408 - val_loss: 0.4583\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4374 - val_loss: 0.4481\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.4416 - val_loss: 0.4363\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4308 - val_loss: 0.4599\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4372 - val_loss: 0.4328\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4305 - val_loss: 0.4556\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4256 - val_loss: 0.4398\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4284 - val_loss: 0.4388\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4298 - val_loss: 0.4507\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4185 - val_loss: 0.4486\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4322 - val_loss: 0.4327\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4397 - val_loss: 0.4507\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4442 - val_loss: 0.4513\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4255 - val_loss: 0.4361\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4167 - val_loss: 0.4600\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4142 - val_loss: 0.4563\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4175 - val_loss: 0.4561\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4349 - val_loss: 0.4532\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4279 - val_loss: 0.4344\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4210 - val_loss: 0.4689\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4155Restoring model weights from the end of the best epoch: 39.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4155 - val_loss: 0.4439\n",
      "Epoch 49: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 96ms/step - loss: 1.6404 - val_loss: 1.0231\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.9517 - val_loss: 0.7024\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7165 - val_loss: 0.6447\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.6062 - val_loss: 0.5445\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5451 - val_loss: 0.4990\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5144 - val_loss: 0.4747\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5028 - val_loss: 0.4867\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4893 - val_loss: 0.4520\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4888 - val_loss: 0.4544\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4892 - val_loss: 0.4493\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4853 - val_loss: 0.4715\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4752 - val_loss: 0.4510\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4657 - val_loss: 0.4555\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4742 - val_loss: 0.4423\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4672 - val_loss: 0.4486\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4652 - val_loss: 0.4621\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4646 - val_loss: 0.4423\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4660 - val_loss: 0.4468\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4701 - val_loss: 0.4550\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4641 - val_loss: 0.4548\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4596 - val_loss: 0.4367\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4539 - val_loss: 0.4373\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4567 - val_loss: 0.4333\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4406 - val_loss: 0.4428\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4440 - val_loss: 0.4529\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4462 - val_loss: 0.4446\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4440 - val_loss: 0.4433\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4339 - val_loss: 0.4572\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4353 - val_loss: 0.4516\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4353 - val_loss: 0.4420\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4363 - val_loss: 0.4413\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4360 - val_loss: 0.4366\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4234Restoring model weights from the end of the best epoch: 23.\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4234 - val_loss: 0.4388\n",
      "Epoch 33: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 45s 540ms/step - loss: 1119602.3750 - val_loss: 226715.8281\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 324127.8750 - val_loss: 183608.5312\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 243869.4219 - val_loss: 700461.9375\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 402726.0625 - val_loss: 343107.0938\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 154998.1094 - val_loss: 114251.1016\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 138188.7500 - val_loss: 185644.1250\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 91891.5312 - val_loss: 238654.4844\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 95884.4766 - val_loss: 99190.5156\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 76367.6172 - val_loss: 53763.2344\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 57737.1367 - val_loss: 62837.3867\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 52437.4609 - val_loss: 58579.6914\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 37417.0273 - val_loss: 17824.1191\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 31360.1172 - val_loss: 63462.1641\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 36984.0430 - val_loss: 48391.8203\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 30591.4121 - val_loss: 70015.3984\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 29592.5527 - val_loss: 61401.6680\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 33143.5195 - val_loss: 79445.0156\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 41179.5469 - val_loss: 23876.5645\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 23456.1445 - val_loss: 37559.4531\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 25941.8008 - val_loss: 12859.4150\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 23536.0566 - val_loss: 45562.4375\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 26254.2344 - val_loss: 8776.1523\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 12636.4502 - val_loss: 7544.8311\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 18690.2070 - val_loss: 25760.9844\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 35766.3477 - val_loss: 57585.6562\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 13726.0420 - val_loss: 30181.4453\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 19797.3828 - val_loss: 8994.5332\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 26065.4727 - val_loss: 9943.9785\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 10865.8223 - val_loss: 15908.9463\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 14204.0039 - val_loss: 24876.0234\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 17913.1582 - val_loss: 16612.3105\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 24297.8008 - val_loss: 33807.1367\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 17164.2070Restoring model weights from the end of the best epoch: 23.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 17164.2070 - val_loss: 12789.0488\n",
      "Epoch 33: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 109ms/step - loss: 866674.8125 - val_loss: 511449.0000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 375233.4375 - val_loss: 485992.5938\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 383608.5938 - val_loss: 354882.0000\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 185645.4219 - val_loss: 117580.2188\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 128041.8438 - val_loss: 133328.5469\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 83066.7734 - val_loss: 70739.0156\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 58418.2578 - val_loss: 38045.9531\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 83521.2422 - val_loss: 37058.8984\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 81129.5078 - val_loss: 239570.3750\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 89044.4609 - val_loss: 19065.3125\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 17728.2949 - val_loss: 74642.7500\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 61417.1914 - val_loss: 22642.9570\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 18464.5547 - val_loss: 99432.1406\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 48599.6680 - val_loss: 22824.6992\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 39964.5547 - val_loss: 52664.9688\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 24241.4023 - val_loss: 65645.1172\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 21914.0586 - val_loss: 27986.3516\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 43155.4375 - val_loss: 5715.3315\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 21141.5410 - val_loss: 19876.7715\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 17077.3809 - val_loss: 15016.3564\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 13090.1172 - val_loss: 10668.0371\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 14939.6504 - val_loss: 40009.2109\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 18936.2871 - val_loss: 42498.2070\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 21110.6250 - val_loss: 24903.4004\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 26356.0352 - val_loss: 10517.0059\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 15341.8906 - val_loss: 46871.7188\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 24727.1973 - val_loss: 23652.2617\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 21745.8555Restoring model weights from the end of the best epoch: 18.\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 21745.8555 - val_loss: 40715.6758\n",
      "Epoch 28: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 766073.1875 - val_loss: 254635.5625\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 308958.4688 - val_loss: 536829.3125\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 203085.7500 - val_loss: 89698.6250\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 104291.6484 - val_loss: 31124.2891\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 59770.3047 - val_loss: 106425.5234\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 89343.3906 - val_loss: 55822.5352\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 45606.2305 - val_loss: 190869.4844\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 82209.5469 - val_loss: 15736.7158\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 52117.3867 - val_loss: 23624.2734\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 41665.0547 - val_loss: 20504.0664\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 19461.9531 - val_loss: 53109.0898\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 39564.5859 - val_loss: 22093.6523\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 27398.0078 - val_loss: 37730.7969\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 33338.9766 - val_loss: 79261.3203\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 24951.3730 - val_loss: 17556.9883\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 22170.6738 - val_loss: 7596.3662\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 26213.6172 - val_loss: 23713.2734\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 22003.5684 - val_loss: 28935.2031\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 22207.2422 - val_loss: 50520.9922\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 23952.4590 - val_loss: 24072.1680\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 19467.3125 - val_loss: 100969.1484\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 44587.3203 - val_loss: 32375.6621\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 25668.0527 - val_loss: 21948.1582\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 15695.8408 - val_loss: 12472.1143\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 14781.8340 - val_loss: 31742.1660\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 15462.2598Restoring model weights from the end of the best epoch: 16.\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 15462.2598 - val_loss: 47574.6523\n",
      "Epoch 26: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 110ms/step - loss: 1925554.7500 - val_loss: 323788.8125\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 299094.4375 - val_loss: 574001.8750\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 255379.4375 - val_loss: 157557.2344\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 282212.5000 - val_loss: 353339.7188\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 110037.5938 - val_loss: 133727.5469\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 78895.3125 - val_loss: 118146.3750\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 118403.2109 - val_loss: 112502.3828\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 66020.1250 - val_loss: 19408.3359\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 51439.2891 - val_loss: 41909.0820\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 38446.6055 - val_loss: 134932.7812\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 76831.3906 - val_loss: 58109.2617\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 61988.4141 - val_loss: 18174.0215\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 39225.7500 - val_loss: 86367.8359\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 70246.0312 - val_loss: 195134.4844\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 56369.9805 - val_loss: 25067.5039\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 31941.1113 - val_loss: 29378.0664\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 25856.9668 - val_loss: 26933.1934\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 22714.4199 - val_loss: 17905.7988\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 25938.4961 - val_loss: 37722.1680\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 19997.1855 - val_loss: 35465.7578\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 27904.5391 - val_loss: 19403.4219\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 17256.9961 - val_loss: 40577.7539\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11908.7188 - val_loss: 83550.5625\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 22338.4805 - val_loss: 39789.5391\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 15635.5068 - val_loss: 11171.8057\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 15537.7783 - val_loss: 14178.2354\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 17561.4473 - val_loss: 23814.1992\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 18821.4082 - val_loss: 4225.9395\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 17241.7949 - val_loss: 20199.1465\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 14510.6074 - val_loss: 44330.4258\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 16254.8672 - val_loss: 15099.2100\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 24793.6562 - val_loss: 39414.1719\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 18339.0723 - val_loss: 31030.7070\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 17600.9297 - val_loss: 10900.1650\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9001.4053 - val_loss: 10487.5254\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 12937.1504 - val_loss: 14799.3154\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 15098.4980 - val_loss: 10248.6035\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 8915.4297Restoring model weights from the end of the best epoch: 28.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8915.4297 - val_loss: 16448.7871\n",
      "Epoch 38: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 657300.6250 - val_loss: 331986.2188\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 600193.7500 - val_loss: 819393.6250\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 280332.0625 - val_loss: 64046.3242\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 96808.2734 - val_loss: 96805.0938\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 67572.2344 - val_loss: 210775.6562\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 133892.5625 - val_loss: 142751.8906\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 91842.1172 - val_loss: 40297.4766\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 45779.5625 - val_loss: 98672.6719\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 55032.4922 - val_loss: 48147.5000\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 55625.6211 - val_loss: 71666.8203\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 33816.5078 - val_loss: 21847.6816\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 49751.1484 - val_loss: 8035.2671\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 47273.3398 - val_loss: 52088.0664\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 28230.1504 - val_loss: 13941.2764\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 25856.0996 - val_loss: 73417.7891\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 26589.7422 - val_loss: 36637.1914\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 36965.8555 - val_loss: 155224.9219\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 49126.2695 - val_loss: 27605.8574\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 21690.4570 - val_loss: 44373.7930\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 23462.6738 - val_loss: 41243.2969\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 25721.0371 - val_loss: 42381.8242\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 11619.0635Restoring model weights from the end of the best epoch: 12.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 11619.0635 - val_loss: 47228.3984\n",
      "Epoch 22: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 98ms/step - loss: 675242.1875 - val_loss: 221739.1719\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 267036.5312 - val_loss: 522015.3438\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 176798.3281 - val_loss: 242747.0000\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 196612.2344 - val_loss: 53737.4922\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 98443.9219 - val_loss: 83491.9219\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 99469.6094 - val_loss: 45493.9570\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 77820.7188 - val_loss: 206973.7969\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 79907.2656 - val_loss: 206491.4219\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 85489.6875 - val_loss: 49971.2539\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 48294.2383 - val_loss: 97331.7500\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 46282.0625 - val_loss: 17583.0645\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 40108.5820 - val_loss: 38939.6211\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 39724.3320 - val_loss: 36100.1523\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 55885.0781 - val_loss: 48603.1211\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 26799.1309 - val_loss: 68532.9453\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 30914.7949 - val_loss: 14394.1992\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 28575.2500 - val_loss: 68730.9844\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 20304.7266 - val_loss: 27282.4004\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 26540.1992 - val_loss: 9578.0469\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 14545.4551 - val_loss: 53153.3438\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 30347.4570 - val_loss: 23986.4238\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 16760.9941 - val_loss: 24159.6992\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 19071.4941 - val_loss: 44245.1133\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 23186.5352 - val_loss: 11204.5762\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 22637.5762 - val_loss: 13962.7227\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 13697.0234 - val_loss: 18632.3301\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 18780.8066 - val_loss: 9194.7246\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 14079.6455 - val_loss: 20595.1191\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 16735.5352 - val_loss: 36861.0547\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 24518.8008 - val_loss: 18195.4492\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 19283.5684 - val_loss: 6703.7183\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 17194.7383 - val_loss: 33195.2188\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 20987.1191 - val_loss: 35501.8398\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 21819.2695 - val_loss: 12164.0146\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 13656.1709 - val_loss: 30351.7070\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 14550.2715 - val_loss: 15100.9502\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6650.7192 - val_loss: 9613.9688\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11203.4141 - val_loss: 2679.7415\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 16349.9131 - val_loss: 40517.7461\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 15302.0225 - val_loss: 5517.3325\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6815.9839 - val_loss: 9676.0537\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7429.7256 - val_loss: 5794.1011\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7447.9766 - val_loss: 21922.5469\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10394.1377 - val_loss: 5568.5688\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8349.9590 - val_loss: 19054.5605\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9435.7520 - val_loss: 14448.2930\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 16197.3271 - val_loss: 10290.5303\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 28426.8359Restoring model weights from the end of the best epoch: 38.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 28426.8359 - val_loss: 23811.7559\n",
      "Epoch 48: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 103ms/step - loss: 1260797.3750 - val_loss: 563828.4375\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 328285.2188 - val_loss: 367954.9062\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 340656.2812 - val_loss: 92971.3203\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 285346.5312 - val_loss: 351382.8438\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 117746.5156 - val_loss: 201749.3750\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 119735.6250 - val_loss: 217929.0781\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 118358.4609 - val_loss: 57715.6211\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 54784.0742 - val_loss: 125220.3438\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 44700.7812 - val_loss: 33777.1953\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 56061.3867 - val_loss: 90544.6328\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 54596.4023 - val_loss: 66932.2656\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 66818.9609 - val_loss: 109779.3203\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33059.0859 - val_loss: 88888.2422\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 42735.5977 - val_loss: 90627.4219\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 48718.0273 - val_loss: 24267.0684\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 24954.6426 - val_loss: 25629.9434\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33668.3945 - val_loss: 45212.9844\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 43037.7422 - val_loss: 77332.5391\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 38646.4023 - val_loss: 14055.6650\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 56817.4375 - val_loss: 51647.2930\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 36672.6328 - val_loss: 29409.6074\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 16686.1973 - val_loss: 19763.6016\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 30126.6270 - val_loss: 17538.1680\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 17422.0234 - val_loss: 32677.6094\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 16147.4033 - val_loss: 37858.5430\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 16191.0674 - val_loss: 27213.5234\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9820.7686 - val_loss: 27707.6992\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 15441.3564 - val_loss: 31059.6133\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 11208.2920Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11208.2920 - val_loss: 55702.7227\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 701441.1875 - val_loss: 807876.4375\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 262664.4375 - val_loss: 291778.9375\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 170140.3438 - val_loss: 737570.5625\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 203125.7344 - val_loss: 100989.6016\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 67441.8984 - val_loss: 116053.7266\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 129521.3281 - val_loss: 86858.6172\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 45370.7500 - val_loss: 19834.7129\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 62580.7188 - val_loss: 45128.0508\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 47630.3867 - val_loss: 61059.7656\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 53288.3125 - val_loss: 103411.1172\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 59775.1914 - val_loss: 84966.7031\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 50191.6328 - val_loss: 10233.9346\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 19245.9297 - val_loss: 17378.8359\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 23053.8926 - val_loss: 27394.5605\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 26116.8301 - val_loss: 63492.0078\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 45869.1953 - val_loss: 6516.7783\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 31751.0371 - val_loss: 51100.1680\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 38689.6250 - val_loss: 27636.7441\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 94265.1953 - val_loss: 275090.0000\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 78732.3750 - val_loss: 28312.1055\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 30457.7363 - val_loss: 26940.5684\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 23737.6523 - val_loss: 2605.1079\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 13066.4365 - val_loss: 13547.4541\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 13841.8066 - val_loss: 30843.7031\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 16551.7461 - val_loss: 71172.8125\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 14545.4238 - val_loss: 22226.1113\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 14978.2842 - val_loss: 19023.7715\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9393.8154 - val_loss: 11712.2939\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10725.2744 - val_loss: 2690.5300\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6886.9180 - val_loss: 20672.8203\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11483.3926 - val_loss: 40670.8164\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 16630.4766Restoring model weights from the end of the best epoch: 22.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 16630.4766 - val_loss: 5862.0728\n",
      "Epoch 32: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 1364770.3750 - val_loss: 294217.6562\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 310367.3750 - val_loss: 104024.7109\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 188616.2656 - val_loss: 78859.1719\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 180249.7500 - val_loss: 304153.1875\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 172616.7656 - val_loss: 102589.2266\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 105399.5156 - val_loss: 191461.7656\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 64914.5898 - val_loss: 126772.8672\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 49081.7656 - val_loss: 232875.4688\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 98372.5781 - val_loss: 227799.1094\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 73883.3906 - val_loss: 124582.7578\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 70435.4219 - val_loss: 13133.9492\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 68508.5312 - val_loss: 96333.7344\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 43953.3750 - val_loss: 17934.2637\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 40643.6094 - val_loss: 49319.7266\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 33707.0586 - val_loss: 58824.5781\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 25953.9336 - val_loss: 19183.0117\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 18972.8984 - val_loss: 55515.0273\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 40621.8320 - val_loss: 97012.2891\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 37459.4570 - val_loss: 91844.2812\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 13234.1914 - val_loss: 17552.1445\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 22208.3262Restoring model weights from the end of the best epoch: 11.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 22208.3262 - val_loss: 31113.1719\n",
      "Epoch 21: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 718098.6875 - val_loss: 114826.1562\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 321546.7188 - val_loss: 407517.5938\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 237398.1719 - val_loss: 247185.7969\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 144627.2656 - val_loss: 330416.6562\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 160420.7812 - val_loss: 93357.5781\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 107189.3672 - val_loss: 157145.6562\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 96669.2422 - val_loss: 288618.1250\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 81316.4453 - val_loss: 50266.0859\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 62448.9688 - val_loss: 132555.6719\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 53335.6875 - val_loss: 60413.1719\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 73981.4297 - val_loss: 59083.6602\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 51125.2617 - val_loss: 66504.2734\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 42681.8750 - val_loss: 75684.2266\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 24196.9492 - val_loss: 220691.6719\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 65009.0742 - val_loss: 62559.3438\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 62357.5977 - val_loss: 19656.8867\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 46118.3359 - val_loss: 44251.5625\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 23723.3281 - val_loss: 93408.2812\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 51991.6875 - val_loss: 75606.2656\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34301.8711 - val_loss: 19568.4102\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 18244.2656 - val_loss: 13709.1680\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 18679.6016 - val_loss: 39524.3008\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 18256.8613 - val_loss: 62962.9023\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 31520.8711 - val_loss: 33984.0312\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 25538.7734 - val_loss: 32743.3516\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 18320.7305 - val_loss: 12193.6924\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 15598.6865 - val_loss: 16999.4023\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10833.4111 - val_loss: 33179.6758\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 13825.0605 - val_loss: 8008.6084\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 16052.3525 - val_loss: 37321.8594\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 19951.4023 - val_loss: 54625.3672\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 50420.5898 - val_loss: 74421.2812\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 31265.9766 - val_loss: 77595.3828\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 38017.1680 - val_loss: 27548.9473\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 16272.3223 - val_loss: 17413.6406\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 14984.3994 - val_loss: 27714.8789\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 17574.2051 - val_loss: 7525.0767\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 14810.0898 - val_loss: 5929.6387\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 17559.0918 - val_loss: 8040.9360\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 12720.6270 - val_loss: 12096.4053\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7596.9126 - val_loss: 7594.6714\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 4758.0122 - val_loss: 8734.2158\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8952.7422 - val_loss: 12857.5625\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10234.8330 - val_loss: 17412.2480\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10702.5107 - val_loss: 15738.8271\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10458.7607 - val_loss: 3902.3411\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7460.7554 - val_loss: 25983.7500\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10702.1758 - val_loss: 18525.0215\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 14213.4824 - val_loss: 13753.7900\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6279.9160 - val_loss: 25050.3574\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9165.0068 - val_loss: 3210.3521\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8717.3291 - val_loss: 13968.8330\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 4747.5059 - val_loss: 7322.4731\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8673.0039 - val_loss: 5422.0928\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6893.4985 - val_loss: 1354.0909\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5137.6655 - val_loss: 10575.3955\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 14996.4980 - val_loss: 4338.7700\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4196.9580 - val_loss: 15134.7568\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5394.4526 - val_loss: 5447.6235\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5907.0479 - val_loss: 6184.1064\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9031.6924 - val_loss: 3667.2803\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6557.7778 - val_loss: 26822.3281\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 11157.1387 - val_loss: 5068.7910\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4820.1436 - val_loss: 21754.2695\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 7887.2974Restoring model weights from the end of the best epoch: 55.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7887.2974 - val_loss: 6418.0708\n",
      "Epoch 65: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 96ms/step - loss: 89.9335 - val_loss: 61.2151\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 63.6861 - val_loss: 50.3462\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 51.4581 - val_loss: 42.1102\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 44.4164 - val_loss: 39.9636\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 42.1769 - val_loss: 35.8082\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 39.1962 - val_loss: 35.5698\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 37.7406 - val_loss: 34.3238\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 36.7741 - val_loss: 33.6667\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 36.3747 - val_loss: 33.5578\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 36.2200 - val_loss: 32.8414\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 35.8935 - val_loss: 33.5783\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 35.6327 - val_loss: 33.0423\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.4914 - val_loss: 33.5119\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 35.6533 - val_loss: 33.1377\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 35.4568 - val_loss: 32.5915\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 35.2125 - val_loss: 32.8185\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 35.1053 - val_loss: 32.4447\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.8634 - val_loss: 32.6461\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.5644 - val_loss: 33.2359\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.9585 - val_loss: 33.4537\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.5855 - val_loss: 32.3433\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.4847 - val_loss: 32.6737\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 34.4317 - val_loss: 31.9363\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 34.1032 - val_loss: 32.2386\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.8382 - val_loss: 32.8039\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.6785 - val_loss: 33.5876\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.2830 - val_loss: 33.4004\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.8748 - val_loss: 32.6925\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.8862 - val_loss: 32.5723\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.4271 - val_loss: 32.4491\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.0906 - val_loss: 32.5056\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.1866 - val_loss: 32.8016\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 33.7919Restoring model weights from the end of the best epoch: 23.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.7919 - val_loss: 32.3912\n",
      "Epoch 33: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 93.7161 - val_loss: 63.9904\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 65.5149 - val_loss: 50.8343\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 53.1087 - val_loss: 42.3361\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 45.5502 - val_loss: 40.2012\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 41.2114 - val_loss: 36.3279\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 39.2756 - val_loss: 33.8013\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 37.5634 - val_loss: 33.4327\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 36.8192 - val_loss: 33.2896\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.9724 - val_loss: 33.6071\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 36.3354 - val_loss: 33.1123\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.4306 - val_loss: 32.5662\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.5571 - val_loss: 34.8098\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 35.8568 - val_loss: 33.6794\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 34.9144 - val_loss: 32.4823\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.5527 - val_loss: 32.6220\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.9908 - val_loss: 32.8021\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.9970 - val_loss: 32.9192\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.8717 - val_loss: 32.3194\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.4673 - val_loss: 32.1395\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.2260 - val_loss: 32.2448\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.4476 - val_loss: 32.9819\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.0281 - val_loss: 31.5389\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.1063 - val_loss: 32.5156\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.6569 - val_loss: 31.5254\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.9786 - val_loss: 31.7327\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.9228 - val_loss: 31.7270\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.8202 - val_loss: 32.3005\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.7928 - val_loss: 34.6623\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.4289 - val_loss: 31.3840\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.8027 - val_loss: 31.4437\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.6463 - val_loss: 31.8680\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.3760 - val_loss: 31.1745\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 34.1079 - val_loss: 32.3392\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 34.3913 - val_loss: 31.6323\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.1492 - val_loss: 33.0253\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.5196 - val_loss: 30.9210\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.3491 - val_loss: 31.0295\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.4960 - val_loss: 31.4300\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.0392 - val_loss: 31.6526\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.1176 - val_loss: 31.7957\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 32.8503 - val_loss: 32.1259\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 32.5523 - val_loss: 31.0842\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 32.5908 - val_loss: 31.4129\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 34.2376 - val_loss: 32.1532\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.5288 - val_loss: 31.4765\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 32.8151Restoring model weights from the end of the best epoch: 36.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 32.8151 - val_loss: 31.5898\n",
      "Epoch 46: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 103ms/step - loss: 189.0909 - val_loss: 192.4749\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 183.6661 - val_loss: 190.6683\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 178.2295 - val_loss: 190.2223\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 177.2417 - val_loss: 190.6458\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 176.8828 - val_loss: 190.8306\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 176.4630 - val_loss: 190.3425\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 176.5302 - val_loss: 189.8958\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 176.1396 - val_loss: 189.8183\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 176.0589 - val_loss: 189.3730\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 178.3829 - val_loss: 188.5171\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 171.1900 - val_loss: 126.5629\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 60.4545 - val_loss: 52.5229\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 48.6749 - val_loss: 49.4110\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 46.7807 - val_loss: 47.6144\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 45.9262 - val_loss: 47.3698\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 45.2330 - val_loss: 48.4438\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 45.1155 - val_loss: 46.9308\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.8896 - val_loss: 47.2456\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 45.0841 - val_loss: 47.4087\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.8404 - val_loss: 46.9427\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.6114 - val_loss: 47.4929\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 45.0828 - val_loss: 47.0574\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.8393 - val_loss: 47.6774\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.6994 - val_loss: 47.0836\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.8888 - val_loss: 46.9770\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 44.7596 - val_loss: 46.9916\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.5900 - val_loss: 46.8858\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.6130 - val_loss: 47.1597\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 44.3454 - val_loss: 47.2113\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.3051 - val_loss: 47.2494\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 44.3467 - val_loss: 46.8491\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.2487 - val_loss: 47.1404\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 44.2878 - val_loss: 46.8277\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.2291 - val_loss: 46.7270\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.2520 - val_loss: 46.8794\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.2867 - val_loss: 46.9410\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.1302 - val_loss: 48.2289\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 43.9935 - val_loss: 47.8189\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 43.9964 - val_loss: 46.8480\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 43.9628 - val_loss: 47.2355\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.1880 - val_loss: 47.5402\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 43.8047 - val_loss: 46.9675\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 43.9640 - val_loss: 46.9109\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 43.6053Restoring model weights from the end of the best epoch: 34.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 43.6053 - val_loss: 47.3146\n",
      "Epoch 44: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 86.2538 - val_loss: 58.6289\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 56.8961 - val_loss: 45.4081\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 47.0057 - val_loss: 39.1956\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 42.7085 - val_loss: 37.5396\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 40.5001 - val_loss: 35.8630\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 38.3947 - val_loss: 33.7998\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 37.2361 - val_loss: 34.6340\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 36.4631 - val_loss: 34.4895\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.8674 - val_loss: 33.7945\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.8214 - val_loss: 33.7837\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 35.7457 - val_loss: 33.6489\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.2448 - val_loss: 33.0704\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.2788 - val_loss: 32.7050\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.1158 - val_loss: 32.3847\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.9143 - val_loss: 32.9771\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.7967 - val_loss: 32.8083\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.0857 - val_loss: 32.5244\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.8287 - val_loss: 32.9615\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.9497 - val_loss: 32.9676\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.6965 - val_loss: 33.1161\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.3612 - val_loss: 32.6766\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.6892 - val_loss: 32.3620\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.5944 - val_loss: 32.4288\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.8060 - val_loss: 32.7235\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.0292 - val_loss: 32.9363\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.1920 - val_loss: 33.8515\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.9817 - val_loss: 33.4508\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.9552 - val_loss: 32.4714\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.7700 - val_loss: 32.2733\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 35.6764 - val_loss: 32.5446\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.7405 - val_loss: 33.2027\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.8803 - val_loss: 33.1033\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.7748 - val_loss: 33.5438\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.4786 - val_loss: 32.7321\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 33.9310 - val_loss: 32.9231\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.4727 - val_loss: 33.6200\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.8972 - val_loss: 33.8038\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 34.3505 - val_loss: 32.6746\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 33.7417Restoring model weights from the end of the best epoch: 29.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 33.7417 - val_loss: 32.4446\n",
      "Epoch 39: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 102ms/step - loss: 184.9663 - val_loss: 190.2348\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 181.2605 - val_loss: 189.7989\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 181.1686 - val_loss: 189.5883\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 176.9633 - val_loss: 189.9323\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 173.3398 - val_loss: 174.0247\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 74.1563 - val_loss: 55.4660\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 51.4736 - val_loss: 50.5807\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 48.1628 - val_loss: 49.1932\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 46.9688 - val_loss: 48.1275\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 46.1086 - val_loss: 48.3279\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 45.7044 - val_loss: 47.3430\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 45.4149 - val_loss: 47.1330\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 44.9601 - val_loss: 47.2485\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.9923 - val_loss: 46.4797\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.8871 - val_loss: 47.2611\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 44.9900 - val_loss: 47.5365\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.8933 - val_loss: 47.1808\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.8621 - val_loss: 47.1572\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.6042 - val_loss: 47.1285\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.4175 - val_loss: 46.8034\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.4271 - val_loss: 46.8108\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.5270 - val_loss: 47.5318\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.6464 - val_loss: 46.9723\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 44.9568Restoring model weights from the end of the best epoch: 14.\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.9568 - val_loss: 47.1589\n",
      "Epoch 24: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 101ms/step - loss: 87.9017 - val_loss: 57.3963\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 60.1989 - val_loss: 47.6366\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 48.2188 - val_loss: 41.7252\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 42.2993 - val_loss: 36.1891\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 39.2324 - val_loss: 34.4878\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 37.3701 - val_loss: 34.3475\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 36.6912 - val_loss: 33.9163\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 36.5427 - val_loss: 33.8754\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 36.2860 - val_loss: 32.3935\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.5966 - val_loss: 33.6057\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.9975 - val_loss: 34.3184\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.8470 - val_loss: 35.0008\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 36.0909 - val_loss: 33.2532\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.6926 - val_loss: 33.6262\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.2350 - val_loss: 34.2196\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.2865 - val_loss: 32.7906\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.9917 - val_loss: 33.7642\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.5259 - val_loss: 32.9735\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.1571 - val_loss: 32.2876\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 35.1697 - val_loss: 32.4720\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.4727 - val_loss: 33.1605\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.4819 - val_loss: 32.4601\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.5070 - val_loss: 33.2840\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.6962 - val_loss: 33.5568\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.1183 - val_loss: 32.6860\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.8715 - val_loss: 32.3068\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.3297 - val_loss: 32.5141\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.0329 - val_loss: 32.5995\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.4515 - val_loss: 32.1293\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.6392 - val_loss: 31.3261\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.1484 - val_loss: 31.5877\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 33.7918 - val_loss: 31.7586\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.0865 - val_loss: 31.9005\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.2729 - val_loss: 32.5026\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.9839 - val_loss: 31.9357\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33.8594 - val_loss: 31.6012\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.5999 - val_loss: 31.9164\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.2714 - val_loss: 31.8999\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.1736 - val_loss: 33.3856\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.3685 - val_loss: 31.0912\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 32.6149 - val_loss: 31.2018\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.0868 - val_loss: 32.5273\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.0279 - val_loss: 32.4251\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.4645 - val_loss: 31.1637\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.4863 - val_loss: 32.3197\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 32.0893 - val_loss: 32.0910\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 32.4407 - val_loss: 31.4830\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.9320 - val_loss: 31.6838\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 32.7309 - val_loss: 31.1412\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 32.6092Restoring model weights from the end of the best epoch: 40.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 32.6092 - val_loss: 31.2987\n",
      "Epoch 50: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 99ms/step - loss: 86.0909 - val_loss: 58.5588\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 56.6743 - val_loss: 45.1124\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 46.2045 - val_loss: 40.4973\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 41.6165 - val_loss: 36.0638\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 39.2140 - val_loss: 34.5251\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 37.5599 - val_loss: 33.6357\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 37.1089 - val_loss: 33.6222\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 36.4004 - val_loss: 33.9022\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 36.1043 - val_loss: 34.0575\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 36.1521 - val_loss: 32.7301\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 35.6205 - val_loss: 32.8341\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 35.2592 - val_loss: 32.6831\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 35.3525 - val_loss: 32.8723\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 35.0357 - val_loss: 32.5019\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 35.1081 - val_loss: 32.1990\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 34.9488 - val_loss: 32.4520\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 34.6475 - val_loss: 32.8600\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.7637 - val_loss: 32.6571\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 34.8530 - val_loss: 33.1560\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.5090 - val_loss: 32.8929\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 34.3229 - val_loss: 32.4421\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.3405 - val_loss: 32.8217\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 35.8776 - val_loss: 34.1172\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 35.0606 - val_loss: 32.4446\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 34.6680Restoring model weights from the end of the best epoch: 15.\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.6680 - val_loss: 32.4655\n",
      "Epoch 25: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 102ms/step - loss: 83.0695 - val_loss: 58.0321\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 55.9515 - val_loss: 46.8553\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 47.1825 - val_loss: 39.5601\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 42.1618 - val_loss: 36.3290\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 39.4982 - val_loss: 35.6529\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 9s 131ms/step - loss: 38.6843 - val_loss: 33.8673\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 36.9059 - val_loss: 35.4647\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 9s 130ms/step - loss: 36.5896 - val_loss: 33.3402\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 36.0529 - val_loss: 34.1114\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 35.4642 - val_loss: 33.0528\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 10s 132ms/step - loss: 35.7024 - val_loss: 33.6459\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 10s 137ms/step - loss: 35.7896 - val_loss: 33.4572\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 23s 319ms/step - loss: 35.3572 - val_loss: 33.4145\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 35.3439 - val_loss: 32.7917\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.9663 - val_loss: 32.3817\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 35.0101 - val_loss: 32.4896\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 34.8206 - val_loss: 33.6246\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.6308 - val_loss: 33.0084\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.4429 - val_loss: 32.3080\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.6797 - val_loss: 32.3744\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.3481 - val_loss: 32.5075\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.3473 - val_loss: 33.4731\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.2744 - val_loss: 33.1460\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 34.3952 - val_loss: 34.9734\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.8077 - val_loss: 32.8007\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 33.9142 - val_loss: 32.5281\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 34.3815 - val_loss: 32.8976\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 33.9124 - val_loss: 33.5497\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 34.0262Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.0262 - val_loss: 32.5143\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 100ms/step - loss: 186.6551 - val_loss: 189.3904\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 138.5292 - val_loss: 68.7408\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 58.5767 - val_loss: 54.2126\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 50.5630 - val_loss: 50.1516\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 47.9457 - val_loss: 48.4858\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 46.9157 - val_loss: 48.3440\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 46.1800 - val_loss: 47.8168\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 45.8798 - val_loss: 47.5320\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 45.4163 - val_loss: 47.2433\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 45.5720 - val_loss: 47.4209\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 45.1352 - val_loss: 47.0085\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.9874 - val_loss: 48.2818\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 45.2348 - val_loss: 47.4707\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.8678 - val_loss: 46.7748\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 44.8475 - val_loss: 47.2821\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 44.5662 - val_loss: 48.3121\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.7739 - val_loss: 47.2639\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 45.0324 - val_loss: 47.5466\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 44.5751 - val_loss: 47.5728\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 44.3973 - val_loss: 47.9349\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 44.5998 - val_loss: 47.2608\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 44.3930 - val_loss: 47.0409\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.5165 - val_loss: 47.2078\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 44.6903Restoring model weights from the end of the best epoch: 14.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 44.6903 - val_loss: 46.8427\n",
      "Epoch 24: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 100ms/step - loss: 189.5532 - val_loss: 191.1219\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 187.0258 - val_loss: 185.9603\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 184.1263 - val_loss: 186.4416\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 184.9221 - val_loss: 182.6936\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 111.4470 - val_loss: 55.3864\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 55.8582 - val_loss: 52.6451\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 49.6009 - val_loss: 50.0136\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 47.8906 - val_loss: 48.5534\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 46.5878 - val_loss: 47.8447\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 45.9697 - val_loss: 47.6620\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 45.9594 - val_loss: 47.8954\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 45.4389 - val_loss: 47.8623\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 45.8654 - val_loss: 47.6373\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 45.5562 - val_loss: 47.8250\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 45.2207 - val_loss: 46.9687\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 45.4082 - val_loss: 47.6925\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 45.0372 - val_loss: 49.5333\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 45.1933 - val_loss: 47.9207\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.9856 - val_loss: 47.2071\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 44.9955 - val_loss: 47.1859\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.9501 - val_loss: 46.8744\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.8852 - val_loss: 47.8119\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.8682 - val_loss: 46.7628\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.8877 - val_loss: 46.5343\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 45.1027 - val_loss: 47.5499\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.8189 - val_loss: 46.8332\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.8204 - val_loss: 46.9235\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.8645 - val_loss: 48.3834\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 45.1218 - val_loss: 47.2075\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.8979 - val_loss: 47.4608\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.6346 - val_loss: 47.4169\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 44.6373 - val_loss: 47.1637\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.7336 - val_loss: 47.1888\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 44.7469Restoring model weights from the end of the best epoch: 24.\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 44.7469 - val_loss: 47.0728\n",
      "Epoch 34: early stopping\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),2000,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',2000,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),2000,10,8,0.001)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "#mae_models = train_bagging_models(model_num, 'mae',2000,10,8,0.001)\n",
    "#mse_models = train_bagging_models(model_num, 'mse',2000,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea5d27a-1d7b-42a9-9f66-76446e61d988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7334.5719537734985"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35de203-07f8-48f9-8ede-0a0bcba1ea27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 30ms/step\n",
      "5/5 [==============================] - 1s 31ms/step\n",
      "5/5 [==============================] - 1s 33ms/step\n",
      "5/5 [==============================] - 1s 33ms/step\n",
      "5/5 [==============================] - 1s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 1s 31ms/step\n",
      "5/5 [==============================] - 1s 32ms/step\n",
      "5/5 [==============================] - 1s 33ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 1s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 1s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 1s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 1s 31ms/step\n",
      "5/5 [==============================] - 1s 32ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 1s 31ms/step\n",
      "5/5 [==============================] - 1s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 1s 32ms/step\n",
      "5/5 [==============================] - 1s 32ms/step\n",
      "5/5 [==============================] - 1s 30ms/step\n",
      "5/5 [==============================] - 1s 30ms/step\n",
      "5/5 [==============================] - 1s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, X_train_val)\n",
    "mase_predictions =  bagging_predict2(pred2, X_train_val)\n",
    "mape_predictions =  bagging_predict2(pred3, X_train_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d34d7f8-6cad-4fb9-b60b-9c4e03aba405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "original 10.37563 2.37769\n",
      "mase 10.0115 2.31159\n",
      "mape 324.9115 15.42686\n",
      "smape 11.52244 2.47043\n",
      "10.389757\n",
      "2.366253\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "\n",
    "smape_predictions_G = bagging_predict2(pred1, test_X)\n",
    "mase_predictions_G = bagging_predict2(pred2,test_X)\n",
    "mape_predictions_G = bagging_predict2(pred3,test_X)\n",
    "print('############################################################################################') \n",
    "print('############################################################################################') \n",
    "\n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G, mase_predictions_G,mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('original',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "pd.DataFrame(fin_pred_G.flatten()).to_csv('lstm.csv')\n",
    "\n",
    "\n",
    "concat_G = np.concatenate([mase_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mase',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mape',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('smape',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "\n",
    "print(np.mean([mean_squared_error(test_y.flatten(),mase_predictions_G[i].flatten()) for i in range(10)]))\n",
    "print(np.mean([mean_absolute_error(test_y.flatten(),mase_predictions_G[i].flatten()) for i in range(10)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

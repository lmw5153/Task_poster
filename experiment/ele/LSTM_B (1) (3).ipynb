{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526bc5-cd1a-4fac-a796-57902266c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636876fb-747b-4480-8ea0-8ff0618bd573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 00:08:13.772935: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 00:08:13.951674: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-01 00:08:13.951692: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-01 00:08:14.643826: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-01 00:08:14.643935: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-01 00:08:14.643947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3850e1af-7ef4-47f2-b130-6732c47014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = 'ele'\n",
    "target_X= pd.read_csv(f\"../data/{data}_train_input_7.csv\").iloc[:,1:].values.astype(np.float32) /10000\n",
    "target_y =pd.read_csv(f\"../data/{data}_train_output_7.csv\").iloc[:,1:].values.astype(np.float32) /10000\n",
    "\n",
    "X_train = target_X[:-round(target_X.shape[0]*0.2),:].astype(np.float32)\n",
    "y_train = target_y[:-round(target_y.shape[0]*0.2)].astype(np.float32)\n",
    "\n",
    "X_train_val= target_X[-round(target_X.shape[0]*0.2):,:].astype(np.float32)\n",
    "y_train_val =target_y[-round(target_y.shape[0]*0.2):].astype(np.float32)\n",
    "\n",
    "\n",
    "test_X= pd.read_csv(f\"../data/{data}_val_input_7.csv\").iloc[:,1:].values.astype(np.float32)/10000\n",
    "test_y =pd.read_csv(f\"../data/{data}_val_output_7.csv\").iloc[:,1:].values.astype(np.float32)/10000\n",
    "#y_train.astype(np.float32)\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3aff672-3e0d-4a08-9d95-0a0eacbdbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],1,y_train.shape[1]\n",
    "\n",
    "#################################################################################\n",
    "# nbeats 모델 생성 함수\n",
    "def build_model(input_timesteps,features,output_timesteps):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(input_timesteps, features)))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    #model.add(LSTM(unit, return_sequences=True))\n",
    "    # Use Lambda layer to select the last 'output_timesteps' outputs\n",
    "    model.add(Lambda(lambda x: x[:, -24:, :]))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "#################################################################################\n",
    "# 부트스트랩 샘플링\n",
    "# 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    input_timesteps,features,output_timesteps= hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = build_model(input_timesteps,features,output_timesteps)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                validation_data = [X_train_val,y_train_val])\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce9eb03-7500-4d76-af24-72f76cc67df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 00:08:18.295553: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-11-01 00:08:18.295611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-11-01 00:08:18.296254: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 89ms/step - loss: 1.4980 - val_loss: 0.9220\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.8605 - val_loss: 0.8039\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.6602 - val_loss: 0.5865\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.5728 - val_loss: 0.5020\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.5569 - val_loss: 0.5895\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.5724 - val_loss: 0.4767\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.5275 - val_loss: 0.5363\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.5170 - val_loss: 0.4741\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.4958 - val_loss: 0.4656\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.5087 - val_loss: 0.4844\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.5076 - val_loss: 0.4681\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.5010 - val_loss: 0.4602\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4918 - val_loss: 0.5195\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4873 - val_loss: 0.4473\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.5005 - val_loss: 0.4567\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.5033 - val_loss: 0.4826\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.5124 - val_loss: 0.5337\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4871 - val_loss: 0.4540\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4754 - val_loss: 0.4842\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4963 - val_loss: 0.4360\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.5009 - val_loss: 0.5147\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4859 - val_loss: 0.4341\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4689 - val_loss: 0.4375\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4703 - val_loss: 0.4547\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4734 - val_loss: 0.4528\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4785 - val_loss: 0.4694\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4934 - val_loss: 0.4561\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.5009 - val_loss: 0.4879\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4463 - val_loss: 0.3876\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4064 - val_loss: 0.4004\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3962 - val_loss: 0.5277\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4232 - val_loss: 0.3666\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4491 - val_loss: 0.4685\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4755 - val_loss: 0.5810\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4678 - val_loss: 0.4979\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4428 - val_loss: 0.5361\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4034 - val_loss: 0.4408\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4003 - val_loss: 0.3701\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3634 - val_loss: 0.3388\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3789 - val_loss: 0.3688\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3442 - val_loss: 0.3411\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3970 - val_loss: 0.3535\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3434 - val_loss: 0.3502\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3277 - val_loss: 0.3398\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3366 - val_loss: 0.3179\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3148 - val_loss: 0.3622\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3994 - val_loss: 0.4322\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3497 - val_loss: 0.3509\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3475 - val_loss: 0.4302\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3453 - val_loss: 0.3723\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3361 - val_loss: 0.3129\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3367 - val_loss: 0.3246\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3332 - val_loss: 0.3637\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2985 - val_loss: 0.2947\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3078 - val_loss: 0.3479\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3589 - val_loss: 0.3678\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3279 - val_loss: 0.3153\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4191 - val_loss: 0.3494\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3422 - val_loss: 0.3167\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.3903 - val_loss: 0.5552\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.5540 - val_loss: 0.4814\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.5125 - val_loss: 0.4546\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4641 - val_loss: 0.6413\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4560Restoring model weights from the end of the best epoch: 54.\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4560 - val_loss: 0.3968\n",
      "Epoch 64: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 1.5401 - val_loss: 0.9816\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8911 - val_loss: 0.8702\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7687 - val_loss: 0.6361\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.6259 - val_loss: 0.5887\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.5665 - val_loss: 0.5244\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.5388 - val_loss: 0.4923\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5229 - val_loss: 0.5360\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5180 - val_loss: 0.5709\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5482 - val_loss: 0.4748\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.5470 - val_loss: 0.5443\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5135 - val_loss: 0.4996\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5097 - val_loss: 0.4999\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5146 - val_loss: 0.4603\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5112 - val_loss: 0.4716\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5064 - val_loss: 0.4837\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5208 - val_loss: 0.5650\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5033 - val_loss: 0.4599\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4934 - val_loss: 0.4775\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4982 - val_loss: 0.4846\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4997 - val_loss: 0.4879\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4916 - val_loss: 0.4560\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4808 - val_loss: 0.4599\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4894 - val_loss: 0.4515\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4870 - val_loss: 0.4698\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4893 - val_loss: 0.4897\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4611 - val_loss: 0.5095\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4952 - val_loss: 0.4339\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4609 - val_loss: 0.4317\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4074 - val_loss: 0.4112\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.3961 - val_loss: 0.3682\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3629 - val_loss: 0.3562\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3478 - val_loss: 0.3433\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3516 - val_loss: 0.3542\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3394 - val_loss: 0.3652\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3545 - val_loss: 0.3213\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3415 - val_loss: 0.3475\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3436 - val_loss: 0.4111\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3213 - val_loss: 0.3186\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3045 - val_loss: 0.2958\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2991 - val_loss: 0.2821\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3054 - val_loss: 0.2918\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2960 - val_loss: 0.2905\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2919 - val_loss: 0.2992\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3161 - val_loss: 0.3819\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3358 - val_loss: 0.3500\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3088 - val_loss: 0.3041\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3002 - val_loss: 0.2909\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3116 - val_loss: 0.3001\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3077 - val_loss: 0.3171\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.3096Restoring model weights from the end of the best epoch: 40.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3096 - val_loss: 0.2990\n",
      "Epoch 50: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 1.5271 - val_loss: 0.9840\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.9005 - val_loss: 0.8885\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7903 - val_loss: 0.6646\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.6149 - val_loss: 0.5614\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5840 - val_loss: 0.5074\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5355 - val_loss: 0.4966\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5260 - val_loss: 0.4861\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5242 - val_loss: 0.4987\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5475 - val_loss: 0.4772\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5137 - val_loss: 0.4643\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4985 - val_loss: 0.4978\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4936 - val_loss: 0.5106\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5025 - val_loss: 0.4632\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4971 - val_loss: 0.4938\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5090 - val_loss: 0.4565\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4828 - val_loss: 0.4586\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4903 - val_loss: 0.5337\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4832 - val_loss: 0.4776\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4722 - val_loss: 0.4369\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4859 - val_loss: 0.5016\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4196 - val_loss: 0.3755\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4061 - val_loss: 0.4554\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4035 - val_loss: 0.4650\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4427 - val_loss: 0.3535\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3890 - val_loss: 0.3597\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3716 - val_loss: 0.3454\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3758 - val_loss: 0.3741\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3547 - val_loss: 0.3541\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3085 - val_loss: 0.3281\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3502 - val_loss: 0.3298\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3114 - val_loss: 0.3383\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3405 - val_loss: 0.3457\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3088 - val_loss: 0.2840\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2966 - val_loss: 0.3970\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3316 - val_loss: 0.3121\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3099 - val_loss: 0.2872\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2860 - val_loss: 0.3642\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3335 - val_loss: 0.3443\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2736 - val_loss: 0.2657\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2802 - val_loss: 0.3435\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2942 - val_loss: 0.2744\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2749 - val_loss: 0.3330\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2905 - val_loss: 0.2660\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2738 - val_loss: 0.2714\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2889 - val_loss: 0.3250\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2815 - val_loss: 0.2679\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2793 - val_loss: 0.3263\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2790 - val_loss: 0.2990\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2526 - val_loss: 0.2477\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2544 - val_loss: 0.2875\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2530 - val_loss: 0.3535\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3554 - val_loss: 0.2715\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2726 - val_loss: 0.2576\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2580 - val_loss: 0.2639\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2828 - val_loss: 0.2601\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2545 - val_loss: 0.2503\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2792 - val_loss: 0.3879\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2760 - val_loss: 0.3152\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.2767Restoring model weights from the end of the best epoch: 49.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2767 - val_loss: 0.2863\n",
      "Epoch 59: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 1.4301 - val_loss: 0.9509\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.8595 - val_loss: 0.7527\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.6615 - val_loss: 0.5781\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5988 - val_loss: 0.5897\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5753 - val_loss: 0.5304\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.5363 - val_loss: 0.5535\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5591 - val_loss: 0.7649\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5649 - val_loss: 0.5799\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.5278 - val_loss: 0.5666\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5138 - val_loss: 0.5800\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5017 - val_loss: 0.4860\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.5128 - val_loss: 0.5053\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.5183 - val_loss: 0.4986\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.5007 - val_loss: 0.4987\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4944 - val_loss: 0.4540\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4781 - val_loss: 0.4743\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4863 - val_loss: 0.4450\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4656 - val_loss: 0.4531\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4969 - val_loss: 0.4523\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4876 - val_loss: 0.4964\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5066 - val_loss: 0.4605\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4718 - val_loss: 0.4397\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4664 - val_loss: 0.4504\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4541 - val_loss: 0.4754\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4499 - val_loss: 0.5125\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4807 - val_loss: 0.4493\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4573 - val_loss: 0.4922\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4364 - val_loss: 0.4316\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4290 - val_loss: 0.4232\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4109 - val_loss: 0.3619\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4414 - val_loss: 0.3991\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3786 - val_loss: 0.3509\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3556 - val_loss: 0.3813\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3417 - val_loss: 0.3387\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3307 - val_loss: 0.3146\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3224 - val_loss: 0.3270\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3516 - val_loss: 0.3524\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3214 - val_loss: 0.3054\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3070 - val_loss: 0.2879\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3058 - val_loss: 0.2983\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2982 - val_loss: 0.3235\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3280 - val_loss: 0.3013\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2866 - val_loss: 0.3123\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3826 - val_loss: 0.3511\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2932 - val_loss: 0.2880\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4429 - val_loss: 0.4043\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4085 - val_loss: 0.3775\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3401 - val_loss: 0.4049\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.3362Restoring model weights from the end of the best epoch: 39.\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3362 - val_loss: 0.3158\n",
      "Epoch 49: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 1.4584 - val_loss: 0.9099\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.8151 - val_loss: 0.7085\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.6824 - val_loss: 0.5871\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.6086 - val_loss: 0.5395\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.5488 - val_loss: 0.5309\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5511 - val_loss: 0.4903\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5400 - val_loss: 0.5468\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5088 - val_loss: 0.4748\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5134 - val_loss: 0.4683\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5189 - val_loss: 0.4755\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5236 - val_loss: 0.5253\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5446 - val_loss: 0.5229\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5008 - val_loss: 0.4624\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4945 - val_loss: 0.4806\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4694 - val_loss: 0.4785\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5020 - val_loss: 0.4572\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4843 - val_loss: 0.4773\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5554 - val_loss: 0.4967\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4806 - val_loss: 0.4563\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4438 - val_loss: 0.4470\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4537 - val_loss: 0.4153\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4152 - val_loss: 0.4516\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4001 - val_loss: 0.3510\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4838 - val_loss: 0.5346\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5292 - val_loss: 0.4516\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4665 - val_loss: 0.4311\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5185 - val_loss: 0.5091\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5329 - val_loss: 0.4873\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5204 - val_loss: 0.4764\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5263 - val_loss: 0.4836\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5199 - val_loss: 0.4697\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5131 - val_loss: 0.4803\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.5056Restoring model weights from the end of the best epoch: 23.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5056 - val_loss: 0.4632\n",
      "Epoch 33: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 1.4414 - val_loss: 0.9410\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.8974 - val_loss: 0.8853\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7303 - val_loss: 0.6537\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.6005 - val_loss: 0.6223\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5817 - val_loss: 0.5477\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5555 - val_loss: 0.4886\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5469 - val_loss: 0.4946\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.6028 - val_loss: 0.4953\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5187 - val_loss: 0.4952\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5130 - val_loss: 0.5335\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5516 - val_loss: 0.5193\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.5007 - val_loss: 0.4700\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4875 - val_loss: 0.4794\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5287 - val_loss: 0.6130\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4963 - val_loss: 0.4833\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5023 - val_loss: 0.4789\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.5066 - val_loss: 0.4656\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4989 - val_loss: 0.4494\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4929 - val_loss: 0.4650\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4709 - val_loss: 0.4630\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5030 - val_loss: 0.4830\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5074 - val_loss: 0.4676\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4969 - val_loss: 0.5298\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5216 - val_loss: 0.4839\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4928 - val_loss: 0.4498\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4706 - val_loss: 0.4640\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4613 - val_loss: 0.5115\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.5032Restoring model weights from the end of the best epoch: 18.\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.5032 - val_loss: 0.5003\n",
      "Epoch 28: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 96ms/step - loss: 1.4155 - val_loss: 0.9188\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.8290 - val_loss: 0.8853\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7448 - val_loss: 0.6206\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.6070 - val_loss: 0.5677\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5569 - val_loss: 0.5100\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5377 - val_loss: 0.5383\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5508 - val_loss: 0.5278\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5088 - val_loss: 0.4747\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5217 - val_loss: 0.4868\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5199 - val_loss: 0.4510\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5099 - val_loss: 0.5124\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.5000 - val_loss: 0.4915\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4929 - val_loss: 0.4802\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5043 - val_loss: 0.4831\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5132 - val_loss: 0.4880\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5209 - val_loss: 0.5101\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4960 - val_loss: 0.4643\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4702 - val_loss: 0.4566\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4801 - val_loss: 0.4674\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4957 - val_loss: 0.4133\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4613 - val_loss: 0.4433\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4259 - val_loss: 0.4358\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4731 - val_loss: 0.4469\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4812 - val_loss: 0.4710\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4910 - val_loss: 0.4270\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4580 - val_loss: 0.4844\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4932 - val_loss: 0.4801\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4767 - val_loss: 0.4237\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4948 - val_loss: 0.4909\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4546Restoring model weights from the end of the best epoch: 20.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4546 - val_loss: 0.4462\n",
      "Epoch 30: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 100ms/step - loss: 1.3538 - val_loss: 0.9189\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.8035 - val_loss: 0.6856\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.6428 - val_loss: 0.5539\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5614 - val_loss: 0.5064\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5585 - val_loss: 0.5111\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5458 - val_loss: 0.5484\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5662 - val_loss: 0.5175\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5071 - val_loss: 0.4645\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5149 - val_loss: 0.4775\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5318 - val_loss: 0.4763\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5146 - val_loss: 0.5655\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4997 - val_loss: 0.4573\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4954 - val_loss: 0.7213\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5157 - val_loss: 0.4914\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4800 - val_loss: 0.4396\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5156 - val_loss: 0.4998\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4930 - val_loss: 0.4269\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4523 - val_loss: 0.5245\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5318 - val_loss: 0.4562\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 0.4515 - val_loss: 0.4266\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 9s 130ms/step - loss: 0.4055 - val_loss: 0.3756\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 0.4468 - val_loss: 0.4184\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 10s 131ms/step - loss: 0.4232 - val_loss: 0.3933\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 10s 132ms/step - loss: 0.4272 - val_loss: 0.4227\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 10s 132ms/step - loss: 0.4085 - val_loss: 0.4991\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 0.4750 - val_loss: 0.4477\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 10s 138ms/step - loss: 0.4103 - val_loss: 0.3488\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 23s 315ms/step - loss: 0.3663 - val_loss: 0.3801\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3636 - val_loss: 0.3574\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4007 - val_loss: 0.4480\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4668 - val_loss: 0.4898\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4886 - val_loss: 0.3975\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3933 - val_loss: 0.3542\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3954 - val_loss: 0.3411\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3466 - val_loss: 0.3279\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3494 - val_loss: 0.4029\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3171 - val_loss: 0.3150\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3144 - val_loss: 0.3027\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3104 - val_loss: 0.3070\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3061 - val_loss: 0.2865\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2822 - val_loss: 0.3017\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2899 - val_loss: 0.2793\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2986 - val_loss: 0.2617\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3323 - val_loss: 0.3201\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3134 - val_loss: 0.2959\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2840 - val_loss: 0.2968\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2852 - val_loss: 0.3601\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2850 - val_loss: 0.2773\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2752 - val_loss: 0.2784\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2653 - val_loss: 0.2668\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3298 - val_loss: 0.2787\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2995 - val_loss: 0.3514\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.2823Restoring model weights from the end of the best epoch: 43.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2823 - val_loss: 0.2762\n",
      "Epoch 53: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 1.4451 - val_loss: 0.9771\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.9263 - val_loss: 0.8396\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7120 - val_loss: 0.5567\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6181 - val_loss: 0.5263\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.6017 - val_loss: 0.5235\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5422 - val_loss: 0.5082\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5390 - val_loss: 0.4820\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5121 - val_loss: 0.5414\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5150 - val_loss: 0.4917\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5163 - val_loss: 0.4937\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5076 - val_loss: 0.4703\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5092 - val_loss: 0.5208\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5016 - val_loss: 0.4524\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4992 - val_loss: 0.4534\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4773 - val_loss: 0.4572\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4807 - val_loss: 0.4858\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4746 - val_loss: 0.4635\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4888 - val_loss: 0.4557\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4629 - val_loss: 0.4446\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4708 - val_loss: 0.4202\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4654 - val_loss: 0.4629\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4883 - val_loss: 0.4772\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4875 - val_loss: 0.7021\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5172 - val_loss: 0.4863\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4843 - val_loss: 0.5185\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5102 - val_loss: 0.5283\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4969 - val_loss: 0.5361\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4958 - val_loss: 0.4467\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4947 - val_loss: 0.4609\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4797Restoring model weights from the end of the best epoch: 20.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4797 - val_loss: 0.5039\n",
      "Epoch 30: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 101ms/step - loss: 1.3996 - val_loss: 0.9796\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.8125 - val_loss: 0.7050\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.6209 - val_loss: 0.6459\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.6166 - val_loss: 0.5913\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5397 - val_loss: 0.5396\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5712 - val_loss: 0.4826\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5350 - val_loss: 0.5052\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5543 - val_loss: 0.4864\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5094 - val_loss: 0.4907\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5077 - val_loss: 0.4789\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5371 - val_loss: 0.4638\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5079 - val_loss: 0.4932\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5157 - val_loss: 0.4658\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4978 - val_loss: 0.4568\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4930 - val_loss: 0.4571\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5032 - val_loss: 0.5297\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5123 - val_loss: 0.4480\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4740 - val_loss: 0.4532\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4728 - val_loss: 0.4346\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4712 - val_loss: 0.4945\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.4902 - val_loss: 0.4496\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.4885 - val_loss: 0.4522\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4743 - val_loss: 0.5368\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4884 - val_loss: 0.4412\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4753 - val_loss: 0.4441\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4586 - val_loss: 0.4005\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4474 - val_loss: 0.4576\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4567 - val_loss: 0.5136\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5122 - val_loss: 0.4472\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4512 - val_loss: 0.3975\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4071 - val_loss: 0.3889\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3975 - val_loss: 0.4167\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4847 - val_loss: 0.4854\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5184 - val_loss: 0.6501\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5109 - val_loss: 0.4434\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4815 - val_loss: 0.4215\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5272 - val_loss: 0.4473\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4313 - val_loss: 0.5033\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5032 - val_loss: 0.4517\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4852 - val_loss: 0.4537\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4563Restoring model weights from the end of the best epoch: 31.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4563 - val_loss: 0.3931\n",
      "Epoch 41: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 19.0679 - val_loss: 11.7306\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 10.0850 - val_loss: 7.9537\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.6851 - val_loss: 8.1465\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.5333 - val_loss: 7.4118\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 7.3304 - val_loss: 6.5623\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.6927 - val_loss: 6.6586\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.6046 - val_loss: 6.0337\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.4594 - val_loss: 5.9417\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.8936 - val_loss: 6.2107\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.4647 - val_loss: 6.4167\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.7916 - val_loss: 5.6234\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.4511 - val_loss: 6.2660\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.4811 - val_loss: 6.3686\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.3657 - val_loss: 5.6679\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.2340 - val_loss: 5.7751\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.1433 - val_loss: 6.3832\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 5.6816 - val_loss: 5.9905\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.8692 - val_loss: 4.6553\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 5.5416 - val_loss: 4.7490\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.3610 - val_loss: 6.0818\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.2038 - val_loss: 4.6955\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 5.2826 - val_loss: 4.5530\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.7329 - val_loss: 5.3051\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 5.0832 - val_loss: 4.1659\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.8884 - val_loss: 4.2475\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.7064 - val_loss: 5.0409\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.5230 - val_loss: 5.5081\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.5626 - val_loss: 4.5010\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.3096 - val_loss: 3.7936\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.2926 - val_loss: 4.7382\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.6629 - val_loss: 6.5886\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.6882 - val_loss: 4.3737\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.0736 - val_loss: 4.1779\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.1465 - val_loss: 3.6145\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.0488 - val_loss: 5.1593\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.4397 - val_loss: 5.0160\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.1432 - val_loss: 6.2623\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.1362 - val_loss: 5.6586\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.7448 - val_loss: 5.8520\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.3182 - val_loss: 6.4277\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 5.1571 - val_loss: 4.2321\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.8881 - val_loss: 4.1831\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.7649 - val_loss: 6.6317\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 4.8796Restoring model weights from the end of the best epoch: 34.\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.8796 - val_loss: 4.1261\n",
      "Epoch 44: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 17.1702 - val_loss: 11.3574\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.2947 - val_loss: 9.2466\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.5114 - val_loss: 7.1269\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.2148 - val_loss: 6.7233\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.4941 - val_loss: 7.1390\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.1461 - val_loss: 6.1686\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.7291 - val_loss: 6.1009\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.7304 - val_loss: 6.1175\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.6747 - val_loss: 6.1310\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.5163 - val_loss: 5.9776\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.2721 - val_loss: 6.0135\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.8005 - val_loss: 6.1851\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.6840 - val_loss: 6.4307\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.7775 - val_loss: 6.9195\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2944 - val_loss: 5.7454\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.1024 - val_loss: 5.9427\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.7084 - val_loss: 6.0825\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.3045 - val_loss: 7.2391\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.8400 - val_loss: 5.8145\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.2850 - val_loss: 6.1092\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.1654 - val_loss: 5.7226\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.1593 - val_loss: 5.6514\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.1344 - val_loss: 5.8999\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.7553 - val_loss: 6.6576\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.1902 - val_loss: 8.5262\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.7806 - val_loss: 5.1297\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.8864 - val_loss: 7.5434\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.8316 - val_loss: 5.9748\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.2677 - val_loss: 5.1770\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.0239 - val_loss: 5.1219\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.7286 - val_loss: 4.6065\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.7116 - val_loss: 4.8285\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.5342 - val_loss: 4.4550\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.7058 - val_loss: 4.8129\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.2468 - val_loss: 3.9055\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.5370 - val_loss: 4.5110\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.0143 - val_loss: 3.7277\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.8274 - val_loss: 3.6930\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.7181 - val_loss: 4.4688\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.0194 - val_loss: 3.6538\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.7461 - val_loss: 3.8936\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.0409 - val_loss: 4.5549\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.2901 - val_loss: 4.8287\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.9677 - val_loss: 3.8494\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.0130 - val_loss: 4.1191\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.6113 - val_loss: 3.7400\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.6766 - val_loss: 3.5526\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.6266 - val_loss: 4.2689\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.8115 - val_loss: 3.5026\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.9013 - val_loss: 3.8207\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.9455 - val_loss: 3.6151\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.8458 - val_loss: 3.7340\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.5179 - val_loss: 3.8200\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.4636 - val_loss: 3.7534\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.6346 - val_loss: 3.4965\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.3564 - val_loss: 3.2992\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.3482 - val_loss: 3.2794\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.3617 - val_loss: 3.4527\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.6389 - val_loss: 3.3632\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.2391 - val_loss: 3.5657\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.3352 - val_loss: 3.2340\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.1417 - val_loss: 3.3306\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.3458 - val_loss: 4.3125\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.4620 - val_loss: 3.4239\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.2213 - val_loss: 3.3917\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.2121 - val_loss: 3.4300\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.2023 - val_loss: 3.1112\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.1255 - val_loss: 3.3034\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.1870 - val_loss: 3.1294\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.1919 - val_loss: 3.1484\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.4260 - val_loss: 4.1800\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.3386 - val_loss: 3.2768\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.6041 - val_loss: 3.3939\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.3000 - val_loss: 3.2767\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.2138 - val_loss: 3.3844\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.0889 - val_loss: 3.4228\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 3.0699Restoring model weights from the end of the best epoch: 67.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.0699 - val_loss: 3.2878\n",
      "Epoch 77: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 16.2335 - val_loss: 11.1441\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.4650 - val_loss: 8.6259\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.3878 - val_loss: 7.1467\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.4326 - val_loss: 6.9219\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.2752 - val_loss: 9.5284\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.2999 - val_loss: 8.3435\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.9096 - val_loss: 6.3328\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.7711 - val_loss: 6.0135\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.8349 - val_loss: 6.5904\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.5771 - val_loss: 6.2373\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.6882 - val_loss: 6.9300\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.0553 - val_loss: 5.8089\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.3689 - val_loss: 6.3307\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.2956 - val_loss: 7.0219\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.6029 - val_loss: 6.2970\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.1220 - val_loss: 5.6541\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.8248 - val_loss: 6.2650\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.4668 - val_loss: 6.0866\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.9003 - val_loss: 5.5482\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.0202 - val_loss: 5.3263\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.7437 - val_loss: 5.1614\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.4456 - val_loss: 5.2227\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.9218 - val_loss: 5.0138\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.3837 - val_loss: 6.6199\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.6616 - val_loss: 5.3630\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.2656 - val_loss: 5.7567\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.9148 - val_loss: 4.1471\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.5150 - val_loss: 4.5934\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.3063 - val_loss: 4.1979\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.0386 - val_loss: 3.9761\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.0057 - val_loss: 3.9315\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.9883 - val_loss: 4.3966\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.3446 - val_loss: 3.6830\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.5284 - val_loss: 3.9932\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.8788 - val_loss: 4.1948\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.7166 - val_loss: 3.5217\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.9061 - val_loss: 4.2931\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 3.5770 - val_loss: 3.5601\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.4725 - val_loss: 3.7180\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.4201 - val_loss: 7.8812\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.4782 - val_loss: 6.2142\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.1733 - val_loss: 5.7995\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.1639 - val_loss: 5.4315\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.0599 - val_loss: 5.4516\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.7194 - val_loss: 5.3139\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 5.2234Restoring model weights from the end of the best epoch: 36.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.2234 - val_loss: 5.1467\n",
      "Epoch 46: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 18.2233 - val_loss: 11.7534\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 11.4535 - val_loss: 11.0988\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 10.2183 - val_loss: 9.9629\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 8.2223 - val_loss: 7.0630\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.3287 - val_loss: 7.8438\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.3628 - val_loss: 6.9069\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.1651 - val_loss: 7.1228\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.8085 - val_loss: 6.0538\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.5832 - val_loss: 6.2842\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.8611 - val_loss: 6.7753\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.8495 - val_loss: 6.4908\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.6147 - val_loss: 6.2331\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.6265 - val_loss: 5.8636\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.3992 - val_loss: 5.9234\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.3281 - val_loss: 5.5637\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2908 - val_loss: 5.5713\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.0151 - val_loss: 6.0626\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.1716 - val_loss: 6.0573\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 5.9902 - val_loss: 6.4949\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.9081 - val_loss: 5.8984\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.9888 - val_loss: 5.2354\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4076 - val_loss: 5.6824\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 5.9815 - val_loss: 5.4411\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 5.9276 - val_loss: 5.8210\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.2020 - val_loss: 6.9625\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.8397 - val_loss: 5.3062\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 5.4405 - val_loss: 6.7632\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.8626 - val_loss: 7.1596\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.1821 - val_loss: 5.9187\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.6554 - val_loss: 4.7477\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.0084 - val_loss: 4.7172\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.1113 - val_loss: 5.3190\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.8489 - val_loss: 4.9116\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.7273 - val_loss: 4.3934\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.5241 - val_loss: 5.1457\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.5390 - val_loss: 4.4099\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.2504 - val_loss: 4.4164\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.7426 - val_loss: 4.3085\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.3775 - val_loss: 4.8986\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.0862 - val_loss: 4.1188\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.0784 - val_loss: 3.9430\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.1882 - val_loss: 4.5052\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.9847 - val_loss: 4.0832\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.8344 - val_loss: 4.6655\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.3736 - val_loss: 5.0511\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 5.0798 - val_loss: 4.2568\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.5368 - val_loss: 4.8386\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.1528 - val_loss: 4.6820\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.8808 - val_loss: 5.0831\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.8661 - val_loss: 4.1332\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 4.5031Restoring model weights from the end of the best epoch: 41.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.5031 - val_loss: 4.9330\n",
      "Epoch 51: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 17.9789 - val_loss: 11.7410\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.2067 - val_loss: 10.1516\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.2904 - val_loss: 7.7487\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.4309 - val_loss: 6.5356\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.1595 - val_loss: 6.4291\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.9567 - val_loss: 6.2405\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.5377 - val_loss: 6.6595\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.9263 - val_loss: 6.6287\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.5540 - val_loss: 7.2012\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.7374 - val_loss: 6.3032\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.3940 - val_loss: 5.9676\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.3499 - val_loss: 6.1142\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.5476 - val_loss: 5.9931\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.6363 - val_loss: 5.8313\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.3701 - val_loss: 5.5988\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.1598 - val_loss: 5.5237\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.7242 - val_loss: 5.4255\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.9427 - val_loss: 5.3966\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.9427 - val_loss: 6.3847\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.6801 - val_loss: 5.0330\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.9417 - val_loss: 5.7625\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.1205 - val_loss: 6.3594\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.5124 - val_loss: 4.9106\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.3717 - val_loss: 4.9768\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.2920 - val_loss: 5.1161\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.3055 - val_loss: 4.7298\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.1153 - val_loss: 4.2959\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.7870 - val_loss: 5.2906\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.1137 - val_loss: 4.4342\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.8904 - val_loss: 5.1901\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.6233 - val_loss: 4.8266\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.9054 - val_loss: 6.0612\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.4206 - val_loss: 4.8430\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.9333 - val_loss: 5.1084\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.6001 - val_loss: 4.6249\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.3657 - val_loss: 5.8603\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 6.2751Restoring model weights from the end of the best epoch: 27.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.2751 - val_loss: 6.3234\n",
      "Epoch 37: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 17.6430 - val_loss: 11.5140\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 10.6114 - val_loss: 8.6603\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.8179 - val_loss: 7.6866\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.4268 - val_loss: 6.6251\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.8405 - val_loss: 6.2173\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.8675 - val_loss: 6.4889\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.5900 - val_loss: 5.8783\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.5952 - val_loss: 6.5707\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.6217 - val_loss: 6.6327\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.4364 - val_loss: 7.1201\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.8006 - val_loss: 6.0226\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4449 - val_loss: 5.8448\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.5676 - val_loss: 5.9471\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.5269 - val_loss: 5.9629\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2490 - val_loss: 5.8296\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.1015 - val_loss: 5.5765\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2852 - val_loss: 6.0651\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.7224 - val_loss: 6.4812\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.1741 - val_loss: 5.3586\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.0324 - val_loss: 5.5743\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2294 - val_loss: 5.1920\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.7208 - val_loss: 4.8990\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.7511 - val_loss: 5.7730\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.3979 - val_loss: 5.2778\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.2350 - val_loss: 4.8433\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.7644 - val_loss: 4.6227\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.7703 - val_loss: 4.6275\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.6019 - val_loss: 5.9412\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.4846 - val_loss: 4.9143\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.9191 - val_loss: 4.5165\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.4632 - val_loss: 4.7910\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.2259 - val_loss: 4.0593\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.2138 - val_loss: 4.0556\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.9798 - val_loss: 4.6134\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.1599 - val_loss: 3.7632\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.9343 - val_loss: 4.3915\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.9522 - val_loss: 3.6854\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.0185 - val_loss: 3.9726\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.1792 - val_loss: 3.7747\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.6184 - val_loss: 4.1646\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.7799 - val_loss: 3.9565\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.7306 - val_loss: 3.9866\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.3481 - val_loss: 5.6734\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.3648 - val_loss: 3.7315\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.0123 - val_loss: 4.1787\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.7083 - val_loss: 3.7410\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 3.5428Restoring model weights from the end of the best epoch: 37.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.5428 - val_loss: 3.7011\n",
      "Epoch 47: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 105ms/step - loss: 16.2661 - val_loss: 11.0716\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.7218 - val_loss: 8.0497\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.1003 - val_loss: 6.8451\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7.8920 - val_loss: 10.8443\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.5429 - val_loss: 7.8936\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.1496 - val_loss: 7.0168\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7.2791 - val_loss: 7.3376\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.7919 - val_loss: 6.0102\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.7990 - val_loss: 6.2385\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.5135 - val_loss: 5.9233\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.4013 - val_loss: 6.4317\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.7024 - val_loss: 6.7038\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.4509 - val_loss: 5.8506\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 6.1343 - val_loss: 5.5309\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.7581 - val_loss: 5.9508\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.3646 - val_loss: 6.4645\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.2558 - val_loss: 5.7957\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 6.1907 - val_loss: 5.4759\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.9979 - val_loss: 5.0108\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 5.6003 - val_loss: 5.1045\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 5.1235 - val_loss: 4.3850\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 5.2981 - val_loss: 5.5721\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 5.1472 - val_loss: 4.6748\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 5.4905 - val_loss: 5.1189\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.8673 - val_loss: 4.6401\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.8534 - val_loss: 4.0852\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 4.6707 - val_loss: 4.5851\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 5.0101 - val_loss: 4.4258\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 4.2171 - val_loss: 4.1268\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 4.1342 - val_loss: 4.4525\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.8343 - val_loss: 5.4174\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.4442 - val_loss: 8.9551\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 8.1773 - val_loss: 7.5126\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7.2987 - val_loss: 6.5000\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 6.8493 - val_loss: 7.5167\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 6.9022Restoring model weights from the end of the best epoch: 26.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.9022 - val_loss: 7.4245\n",
      "Epoch 36: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 17.4921 - val_loss: 11.3926\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 10.6236 - val_loss: 8.4315\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 8.0095 - val_loss: 7.1498\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.8348 - val_loss: 6.7826\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 7.1086 - val_loss: 7.1254\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 7.0341 - val_loss: 6.1090\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.7491 - val_loss: 5.9940\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4828 - val_loss: 6.7949\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.7001 - val_loss: 5.9817\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.5753 - val_loss: 6.0631\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.5606 - val_loss: 6.1403\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.1972 - val_loss: 5.7934\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.3106 - val_loss: 5.5010\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.5078 - val_loss: 5.8325\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.4369 - val_loss: 6.6179\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.1978 - val_loss: 6.2689\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2354 - val_loss: 5.5484\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.0960 - val_loss: 6.2926\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.3530 - val_loss: 6.0759\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.9732 - val_loss: 5.6170\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.7421 - val_loss: 5.7703\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 5.9228 - val_loss: 7.1652\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.2346 - val_loss: 5.4396\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.6476 - val_loss: 5.4956\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.7982 - val_loss: 4.9126\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.4564 - val_loss: 5.4063\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.9110 - val_loss: 6.1588\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4940 - val_loss: 5.6938\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.3450 - val_loss: 6.8669\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4903 - val_loss: 5.3987\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.6629 - val_loss: 5.5810\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.0012 - val_loss: 6.2248\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.9201 - val_loss: 5.4030\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.8346 - val_loss: 5.3843\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 6.0053Restoring model weights from the end of the best epoch: 25.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.0053 - val_loss: 5.6713\n",
      "Epoch 35: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 98ms/step - loss: 19.5188 - val_loss: 11.9371\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.4535 - val_loss: 11.5784\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.0367 - val_loss: 8.7693\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.0837 - val_loss: 7.4739\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.6452 - val_loss: 6.7854\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.5238 - val_loss: 8.5160\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.2440 - val_loss: 6.6693\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.6982 - val_loss: 6.6071\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.6999 - val_loss: 6.7424\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.7070 - val_loss: 5.8239\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.1308 - val_loss: 6.4259\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.4311 - val_loss: 6.6559\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.4551 - val_loss: 6.0361\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.4886 - val_loss: 5.8941\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.2687 - val_loss: 6.7058\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.6166 - val_loss: 6.2999\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.3999 - val_loss: 5.9214\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.0873 - val_loss: 6.2473\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.0864 - val_loss: 5.3589\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.2432 - val_loss: 6.5670\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.7983 - val_loss: 6.8428\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.8056 - val_loss: 5.1574\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.2046 - val_loss: 5.3619\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.9253 - val_loss: 6.0789\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.4012 - val_loss: 5.4172\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.3174 - val_loss: 4.7572\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.1415 - val_loss: 4.5783\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.8521 - val_loss: 4.3565\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.5745 - val_loss: 4.4785\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.7038 - val_loss: 4.3057\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.5016 - val_loss: 3.9854\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.3660 - val_loss: 4.0313\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.4900 - val_loss: 4.5062\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.4921 - val_loss: 5.3837\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.3593 - val_loss: 4.1065\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.1907 - val_loss: 4.7986\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.2965 - val_loss: 4.2023\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.8712 - val_loss: 3.8207\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.2242 - val_loss: 4.4313\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.0192 - val_loss: 3.7565\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.6932 - val_loss: 4.3801\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.8851 - val_loss: 3.7427\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.9120 - val_loss: 4.4851\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.7281 - val_loss: 3.9828\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.5047 - val_loss: 4.0134\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.6416 - val_loss: 3.8487\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.4645 - val_loss: 3.7811\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.4566 - val_loss: 3.8541\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.4816 - val_loss: 3.5440\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.4936 - val_loss: 3.7911\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.4269 - val_loss: 3.6557\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.5375 - val_loss: 3.6120\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.0203 - val_loss: 6.9117\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.8996 - val_loss: 6.7693\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.7860 - val_loss: 6.6715\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.7669 - val_loss: 6.0995\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.5966 - val_loss: 6.2379\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.6307 - val_loss: 5.8624\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 6.4979Restoring model weights from the end of the best epoch: 49.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.4979 - val_loss: 5.8271\n",
      "Epoch 59: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 17.0371 - val_loss: 11.6340\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 10.6844 - val_loss: 10.6104\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 8.3742 - val_loss: 7.2813\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.6184 - val_loss: 7.7645\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.2018 - val_loss: 6.2910\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.1897 - val_loss: 7.3480\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.9491 - val_loss: 6.2254\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.9925 - val_loss: 6.8143\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.7226 - val_loss: 7.2598\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.1252 - val_loss: 7.5367\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.5984 - val_loss: 6.5200\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.7786 - val_loss: 5.7160\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.5881 - val_loss: 6.0632\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4499 - val_loss: 5.6841\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.3032 - val_loss: 6.1796\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4507 - val_loss: 5.8999\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.4856 - val_loss: 6.4888\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.6120 - val_loss: 6.9263\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.2757 - val_loss: 5.3053\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.0852 - val_loss: 7.2702\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.4380 - val_loss: 6.1810\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.5457 - val_loss: 5.3137\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.4858 - val_loss: 6.3918\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.1544 - val_loss: 4.5313\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.7620 - val_loss: 5.2956\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.7878 - val_loss: 4.6651\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.3847 - val_loss: 4.0963\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.2688 - val_loss: 4.0452\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.5258 - val_loss: 4.9789\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.2952 - val_loss: 3.8624\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.9975 - val_loss: 4.1405\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.7763 - val_loss: 4.5262\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.3197 - val_loss: 4.2748\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.0883 - val_loss: 5.4729\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.6095 - val_loss: 8.1037\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.4892 - val_loss: 4.7469\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.6853 - val_loss: 5.7834\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.6870 - val_loss: 5.7249\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.6740 - val_loss: 5.3264\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 4.3956Restoring model weights from the end of the best epoch: 30.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.3956 - val_loss: 4.2199\n",
      "Epoch 40: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 200.0000Restoring model weights from the end of the best epoch: 1.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 11: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 20.7937 - val_loss: 11.8251\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10.9811 - val_loss: 9.9604\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.7855 - val_loss: 7.3675\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.6190 - val_loss: 8.3316\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 7.3590 - val_loss: 6.4041\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.8876 - val_loss: 6.8519\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7.0142 - val_loss: 7.7287\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.7202 - val_loss: 6.3346\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 6.5918 - val_loss: 6.3753\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.8006 - val_loss: 5.8575\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.4037 - val_loss: 6.4804\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.6193 - val_loss: 6.0465\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.5177 - val_loss: 5.9185\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.2598 - val_loss: 6.2556\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.5842 - val_loss: 7.7774\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.6535 - val_loss: 7.7082\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.3492 - val_loss: 5.9030\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.2051 - val_loss: 6.2200\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.3484 - val_loss: 5.6528\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 6.1442 - val_loss: 6.7148\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 5.7603 - val_loss: 5.3614\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 5.9382 - val_loss: 6.0502\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.6136 - val_loss: 6.2913\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.1764 - val_loss: 6.6329\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.3220 - val_loss: 5.8492\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.2398 - val_loss: 5.5838\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 6.6071 - val_loss: 5.8787\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.2765 - val_loss: 6.4696\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.3208 - val_loss: 7.9560\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7.0181 - val_loss: 6.0936\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 6.0486Restoring model weights from the end of the best epoch: 21.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.0486 - val_loss: 5.4485\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 21.5081 - val_loss: 11.5856\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.8200 - val_loss: 8.8993\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 8.4699 - val_loss: 8.4377\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.7728 - val_loss: 9.0713\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 7.7043 - val_loss: 6.5190\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.1980 - val_loss: 6.4322\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.9079 - val_loss: 6.4781\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.8263 - val_loss: 6.6907\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.6305 - val_loss: 6.3792\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.1560 - val_loss: 8.8625\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.8527 - val_loss: 6.0281\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.4470 - val_loss: 6.3231\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.5816 - val_loss: 6.0168\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.3305 - val_loss: 5.9014\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.1877 - val_loss: 5.7509\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.3105 - val_loss: 6.7091\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.6796 - val_loss: 6.6273\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.5068 - val_loss: 5.9825\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.4268 - val_loss: 6.5177\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.1615 - val_loss: 5.7490\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.1837 - val_loss: 5.6530\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.1444 - val_loss: 6.6969\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.3628 - val_loss: 6.4771\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2958 - val_loss: 5.8334\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.2033 - val_loss: 6.0210\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.5266 - val_loss: 5.6785\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 5.6449 - val_loss: 8.1274\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.1228 - val_loss: 6.4913\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.5264 - val_loss: 5.9799\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 5.9365 - val_loss: 5.5814\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.9832 - val_loss: 5.2313\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.3813 - val_loss: 5.9057\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 5.2151 - val_loss: 4.9382\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.0645 - val_loss: 4.6307\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.5998 - val_loss: 4.5042\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.5006 - val_loss: 4.2242\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.4101 - val_loss: 4.6707\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.5619 - val_loss: 3.7642\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.1522 - val_loss: 3.8327\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.0328 - val_loss: 4.6458\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 5.1403 - val_loss: 4.5400\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.2228 - val_loss: 3.9807\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.0516 - val_loss: 3.9586\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.0902 - val_loss: 4.5309\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.9234 - val_loss: 4.9319\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.3499 - val_loss: 4.4990\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.8301 - val_loss: 3.5620\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.5922 - val_loss: 3.8997\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.7660 - val_loss: 3.6576\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.7553 - val_loss: 3.9123\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.5329 - val_loss: 3.4372\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.5490 - val_loss: 3.9059\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.3733 - val_loss: 3.6154\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.6041 - val_loss: 3.2514\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.4281 - val_loss: 3.3359\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.6383 - val_loss: 3.4465\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.3569 - val_loss: 3.4937\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.4094 - val_loss: 3.5151\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.3446 - val_loss: 3.6419\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.4706 - val_loss: 3.3709\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.4086 - val_loss: 3.7766\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.7447 - val_loss: 4.3601\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.7315 - val_loss: 3.9037\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 3.9186Restoring model weights from the end of the best epoch: 54.\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.9186 - val_loss: 3.4962\n",
      "Epoch 64: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 21.7917 - val_loss: 12.3234\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.2907 - val_loss: 11.2480\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.1053 - val_loss: 8.3520\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.5263 - val_loss: 8.0964\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.3348 - val_loss: 6.5485\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.2606 - val_loss: 6.7946\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.2161 - val_loss: 6.3053\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.0503 - val_loss: 6.7639\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.4665 - val_loss: 6.4360\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.7239 - val_loss: 6.9990\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.9217 - val_loss: 6.1507\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.6329 - val_loss: 6.0658\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.6256 - val_loss: 5.9608\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.2745 - val_loss: 6.2292\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.2233 - val_loss: 5.8522\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.3988 - val_loss: 6.6345\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.0373 - val_loss: 6.0859\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.1024 - val_loss: 5.1629\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.6733 - val_loss: 6.2382\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.5898 - val_loss: 5.1887\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.8068 - val_loss: 6.3885\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.2433 - val_loss: 4.8065\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.7069 - val_loss: 6.6130\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.7174 - val_loss: 5.9053\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.4097 - val_loss: 6.1095\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.2738 - val_loss: 6.4489\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.0114 - val_loss: 5.5972\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.8438 - val_loss: 5.9228\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.7562 - val_loss: 5.4815\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.3097 - val_loss: 5.1828\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.1828 - val_loss: 5.1432\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.9578 - val_loss: 4.5895\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.8415 - val_loss: 4.8313\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.6970 - val_loss: 4.6626\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.8695 - val_loss: 4.5551\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.8430 - val_loss: 4.7619\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.6873 - val_loss: 4.6639\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.6311 - val_loss: 4.5072\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.8657 - val_loss: 4.2902\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.5818 - val_loss: 4.5387\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.5031 - val_loss: 4.0821\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.5849 - val_loss: 5.0869\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.9445 - val_loss: 4.3641\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.6779 - val_loss: 4.8013\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.0959 - val_loss: 4.7934\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.3732 - val_loss: 4.2242\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.9714 - val_loss: 4.6459\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.6783 - val_loss: 4.4871\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.1540 - val_loss: 3.9522\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.0597 - val_loss: 3.8687\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.2246 - val_loss: 4.1634\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.4363 - val_loss: 4.2005\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.6618 - val_loss: 4.5547\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.5448 - val_loss: 4.0616\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.3310 - val_loss: 3.9758\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.2612 - val_loss: 3.8218\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.1850 - val_loss: 4.4573\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.7233 - val_loss: 4.3735\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.9901 - val_loss: 3.9566\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 4.4763 - val_loss: 4.3844\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.8871 - val_loss: 5.9325\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.4297 - val_loss: 5.7010\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.0973 - val_loss: 5.7431\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.1364 - val_loss: 5.5696\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.9306 - val_loss: 5.5646\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 5.9373Restoring model weights from the end of the best epoch: 56.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.9373 - val_loss: 6.4680\n",
      "Epoch 66: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 23.3382 - val_loss: 11.8889\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.4854 - val_loss: 11.3831\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.5441 - val_loss: 10.6090\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 8.5402 - val_loss: 8.1391\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.9293 - val_loss: 7.6070\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.1727 - val_loss: 7.6501\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.1502 - val_loss: 6.2352\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.0624 - val_loss: 6.5894\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.1072 - val_loss: 6.2637\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.6039 - val_loss: 6.0972\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4287 - val_loss: 6.1553\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.6620 - val_loss: 6.7532\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.4340 - val_loss: 6.0980\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.4840 - val_loss: 6.3558\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.3501 - val_loss: 6.3399\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2762 - val_loss: 6.9605\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.5281 - val_loss: 6.5870\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.0395 - val_loss: 6.2237\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.4047 - val_loss: 7.9590\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 6.5098Restoring model weights from the end of the best epoch: 10.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.5098 - val_loss: 6.2739\n",
      "Epoch 20: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 99ms/step - loss: 24.0574 - val_loss: 11.8022\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 11.3623 - val_loss: 10.7721\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.6892 - val_loss: 8.9458\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.1352 - val_loss: 6.8915\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 7.6193 - val_loss: 7.8995\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.0471 - val_loss: 7.0198\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.3078 - val_loss: 6.7142\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.6676 - val_loss: 6.0428\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.6237 - val_loss: 6.3893\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.1773 - val_loss: 6.0185\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.4110 - val_loss: 7.7969\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.4833 - val_loss: 5.8272\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.5153 - val_loss: 5.9278\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.4064 - val_loss: 5.9310\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.2847 - val_loss: 6.8475\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 6.5281 - val_loss: 7.0011\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 6.4298 - val_loss: 5.8804\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 6.6520 - val_loss: 8.5443\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.5901 - val_loss: 6.8432\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 6.6361 - val_loss: 7.7875\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.4457 - val_loss: 5.9463\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 6.1781Restoring model weights from the end of the best epoch: 12.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.1781 - val_loss: 6.7438\n",
      "Epoch 22: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 24.2652 - val_loss: 11.9957\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.4300 - val_loss: 12.6273\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.6723 - val_loss: 9.3707\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.4290 - val_loss: 7.3741\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.8578 - val_loss: 6.9721\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.3832 - val_loss: 7.3258\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.9132 - val_loss: 6.3404\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.7218 - val_loss: 6.5698\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.6815 - val_loss: 6.4467\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.7926 - val_loss: 6.8657\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.6994 - val_loss: 6.0881\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.8271 - val_loss: 5.9641\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.3755 - val_loss: 6.1722\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.7311 - val_loss: 6.2004\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.4650 - val_loss: 6.3773\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.2122 - val_loss: 6.1847\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.8888 - val_loss: 7.7404\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.6230 - val_loss: 6.0808\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2002 - val_loss: 6.7396\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2862 - val_loss: 6.0388\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.1599 - val_loss: 6.7027\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 6.2361Restoring model weights from the end of the best epoch: 12.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.2361 - val_loss: 6.1587\n",
      "Epoch 22: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 21.2163 - val_loss: 11.8359\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.4634 - val_loss: 11.2313\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.0854 - val_loss: 10.4496\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 8.8646 - val_loss: 8.0100\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.8662 - val_loss: 7.1702\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.0604 - val_loss: 6.3410\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.9076 - val_loss: 7.8053\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.9856 - val_loss: 6.5770\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.8235 - val_loss: 8.1751\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.3059 - val_loss: 6.1131\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.4878 - val_loss: 5.9052\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.5997 - val_loss: 6.2456\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.2620 - val_loss: 6.3406\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.5083 - val_loss: 6.5541\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.3170 - val_loss: 6.0161\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2451 - val_loss: 5.7408\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.3978 - val_loss: 5.7152\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.2542 - val_loss: 6.9790\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.5091 - val_loss: 6.6504\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.3577 - val_loss: 5.9079\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.4079 - val_loss: 6.7361\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2350 - val_loss: 6.3107\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2478 - val_loss: 6.0301\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.1205 - val_loss: 5.8876\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.0155 - val_loss: 5.9336\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.4998 - val_loss: 5.8744\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 6.2266Restoring model weights from the end of the best epoch: 17.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.2266 - val_loss: 5.8568\n",
      "Epoch 27: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 101ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 200.0000Restoring model weights from the end of the best epoch: 1.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 11: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 200.0000Restoring model weights from the end of the best epoch: 1.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 11: early stopping\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),2000,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',2000,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),2000,10,8,0.001)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "#mae_models = train_bagging_models(model_num, 'mae',2000,10,8,0.001)\n",
    "#mse_models = train_bagging_models(model_num, 'mse',2000,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea5d27a-1d7b-42a9-9f66-76446e61d988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8033.056547641754"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35de203-07f8-48f9-8ede-0a0bcba1ea27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 33ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 27ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, X_train_val)\n",
    "mase_predictions =  bagging_predict2(pred2, X_train_val)\n",
    "mape_predictions =  bagging_predict2(pred3, X_train_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d34d7f8-6cad-4fb9-b60b-9c4e03aba405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "original 0.15862 0.25145\n",
      "mase 0.15792 0.25723\n",
      "mape 0.13123 0.23182\n",
      "smape 0.27816 0.33717\n",
      "0.20549028\n",
      "0.29262868\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "\n",
    "smape_predictions_G = bagging_predict2(pred1, test_X)\n",
    "mase_predictions_G = bagging_predict2(pred2,test_X)\n",
    "mape_predictions_G = bagging_predict2(pred3,test_X)\n",
    "print('############################################################################################') \n",
    "print('############################################################################################') \n",
    "\n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G, mase_predictions_G,mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('original',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "pd.DataFrame(fin_pred_G.flatten()).to_csv('lstm.csv')\n",
    "\n",
    "\n",
    "concat_G = np.concatenate([mase_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mase',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mape',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('smape',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "\n",
    "print(np.mean([mean_squared_error(test_y.flatten(),mase_predictions_G[i].flatten()) for i in range(10)]))\n",
    "print(np.mean([mean_absolute_error(test_y.flatten(),mase_predictions_G[i].flatten()) for i in range(10)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

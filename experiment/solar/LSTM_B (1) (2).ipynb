{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526bc5-cd1a-4fac-a796-57902266c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636876fb-747b-4480-8ea0-8ff0618bd573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 19:42:12.013588: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 19:42:12.130859: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-31 19:42:12.130883: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-31 19:42:12.633110: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-31 19:42:12.633165: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-31 19:42:12.633171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3850e1af-7ef4-47f2-b130-6732c47014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = 'solor'\n",
    "target_X= pd.read_csv(f\"../data/{data}_train_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "target_y =pd.read_csv(f\"../data/{data}_train_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "X_train = target_X[:-round(target_X.shape[0]*0.2),:].astype(np.float32)\n",
    "y_train = target_y[:-round(target_y.shape[0]*0.2)].astype(np.float32)\n",
    "\n",
    "X_train_val= target_X[-round(target_X.shape[0]*0.2):,:].astype(np.float32)\n",
    "y_train_val =target_y[-round(target_y.shape[0]*0.2):].astype(np.float32)\n",
    "\n",
    "\n",
    "test_X= pd.read_csv(f\"../data/{data}_val_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "test_y =pd.read_csv(f\"../data/{data}_val_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "#y_train.astype(np.float32)\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3aff672-3e0d-4a08-9d95-0a0eacbdbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],1,y_train.shape[1]\n",
    "\n",
    "#################################################################################\n",
    "# nbeats 모델 생성 함수\n",
    "def build_model(input_timesteps,features,output_timesteps):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(input_timesteps, features)))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    #model.add(LSTM(unit, return_sequences=True))\n",
    "    # Use Lambda layer to select the last 'output_timesteps' outputs\n",
    "    model.add(Lambda(lambda x: x[:, -24:, :]))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "#################################################################################\n",
    "# 부트스트랩 샘플링\n",
    "# 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    input_timesteps,features,output_timesteps= hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = build_model(input_timesteps,features,output_timesteps)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                validation_data = [X_train_val,y_train_val])\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce9eb03-7500-4d76-af24-72f76cc67df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 19:42:19.729050: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-10-31 19:42:19.729092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-10-31 19:42:19.729878: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 1.0981 - val_loss: 0.7264\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.8413 - val_loss: 0.6492\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8121 - val_loss: 0.6615\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7851 - val_loss: 0.6949\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.7999 - val_loss: 0.6070\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.7630 - val_loss: 0.6171\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.7728 - val_loss: 0.6258\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.7531 - val_loss: 0.5745\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7485 - val_loss: 0.5881\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.7530 - val_loss: 0.5658\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7565 - val_loss: 0.5839\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7517 - val_loss: 0.5955\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7399 - val_loss: 0.5792\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7409 - val_loss: 0.5626\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7382 - val_loss: 0.6080\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7522 - val_loss: 0.5802\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7411 - val_loss: 0.6349\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7511 - val_loss: 0.5713\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7392 - val_loss: 0.5719\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7259 - val_loss: 0.5733\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7332 - val_loss: 0.5820\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7367 - val_loss: 0.6102\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7339 - val_loss: 0.5730\n",
      "Epoch 24/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7389Restoring model weights from the end of the best epoch: 14.\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7419 - val_loss: 0.5679\n",
      "Epoch 24: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 106ms/step - loss: 1.0952 - val_loss: 0.7308\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.8489 - val_loss: 0.7432\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.8101 - val_loss: 0.6064\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.7868 - val_loss: 0.6337\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.7943 - val_loss: 0.6058\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.7768 - val_loss: 0.5985\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.7683 - val_loss: 0.6032\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.7601 - val_loss: 0.6728\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7625 - val_loss: 0.6156\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7519 - val_loss: 0.6177\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7461 - val_loss: 0.5862\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.7488 - val_loss: 0.5970\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7618 - val_loss: 0.5762\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.7531 - val_loss: 0.5963\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7660 - val_loss: 0.5897\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7363 - val_loss: 0.5780\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7400 - val_loss: 0.5917\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7441 - val_loss: 0.6488\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7401 - val_loss: 0.5961\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.7405 - val_loss: 0.5621\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7324 - val_loss: 0.5903\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.7418 - val_loss: 0.6059\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7297 - val_loss: 0.5950\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7315 - val_loss: 0.6038\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7389 - val_loss: 0.5790\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7360 - val_loss: 0.5563\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7318 - val_loss: 0.5598\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7300 - val_loss: 0.5864\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.7266 - val_loss: 0.5730\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7330 - val_loss: 0.5942\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.7294 - val_loss: 0.5892\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7348 - val_loss: 0.6267\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.7406 - val_loss: 0.6022\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.7333 - val_loss: 0.5691\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.7247 - val_loss: 0.5599\n",
      "Epoch 36/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7281Restoring model weights from the end of the best epoch: 26.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7278 - val_loss: 0.5662\n",
      "Epoch 36: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 99ms/step - loss: 1.1851 - val_loss: 0.7380\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.8351 - val_loss: 0.7339\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8136 - val_loss: 0.6286\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.8241 - val_loss: 0.6676\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.8005 - val_loss: 0.6738\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7803 - val_loss: 0.6137\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7691 - val_loss: 0.6243\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7668 - val_loss: 0.5942\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7520 - val_loss: 0.5947\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7825 - val_loss: 0.5760\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7700 - val_loss: 0.5713\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7418 - val_loss: 0.5939\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7484 - val_loss: 0.5890\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7470 - val_loss: 0.5964\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.7377 - val_loss: 0.5664\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7498 - val_loss: 0.5840\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.7385 - val_loss: 0.5830\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7461 - val_loss: 0.5942\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7443 - val_loss: 0.5945\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7380 - val_loss: 0.5696\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7334 - val_loss: 0.5725\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7287 - val_loss: 0.5870\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7299 - val_loss: 0.5718\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7447 - val_loss: 0.5584\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.7317 - val_loss: 0.5726\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7418 - val_loss: 0.5999\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7302 - val_loss: 0.5631\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7273 - val_loss: 0.5716\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7265 - val_loss: 0.5965\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.7264 - val_loss: 0.5680\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7265 - val_loss: 0.5783\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7228 - val_loss: 0.5684\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7291 - val_loss: 0.5806\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7176 - val_loss: 0.5571\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7224 - val_loss: 0.5815\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7279 - val_loss: 0.5556\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7307 - val_loss: 0.5643\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7279 - val_loss: 0.5902\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7342 - val_loss: 0.5747\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7325 - val_loss: 0.6118\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7271 - val_loss: 0.5895\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7312 - val_loss: 0.6723\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7371 - val_loss: 0.5588\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7164 - val_loss: 0.5970\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7348 - val_loss: 0.5630\n",
      "Epoch 46/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7197Restoring model weights from the end of the best epoch: 36.\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7189 - val_loss: 0.5562\n",
      "Epoch 46: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 1.1051 - val_loss: 0.7133\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.8439 - val_loss: 0.6774\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.8050 - val_loss: 0.6390\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8190 - val_loss: 0.6517\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7794 - val_loss: 0.6042\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7792 - val_loss: 0.6201\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7773 - val_loss: 0.6030\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7634 - val_loss: 0.5994\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7610 - val_loss: 0.6069\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7557 - val_loss: 0.5756\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7432 - val_loss: 0.6279\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7467 - val_loss: 0.5610\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7475 - val_loss: 0.6168\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7578 - val_loss: 0.5998\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7491 - val_loss: 0.5916\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7455 - val_loss: 0.5810\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7444 - val_loss: 0.5858\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7379 - val_loss: 0.5812\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7404 - val_loss: 0.5604\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7263 - val_loss: 0.6318\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7434 - val_loss: 0.5752\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7350 - val_loss: 0.5618\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7329 - val_loss: 0.5716\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7281 - val_loss: 0.6120\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7307 - val_loss: 0.5642\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7289 - val_loss: 0.6164\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7385 - val_loss: 0.5981\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7414 - val_loss: 0.5654\n",
      "Epoch 29/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7437Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7428 - val_loss: 0.5766\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 91ms/step - loss: 1.1107 - val_loss: 0.8014\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.8385 - val_loss: 0.6566\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.8041 - val_loss: 0.6225\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7821 - val_loss: 0.5885\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7798 - val_loss: 0.6193\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7845 - val_loss: 0.6314\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7697 - val_loss: 0.6294\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7657 - val_loss: 0.5861\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7640 - val_loss: 0.6212\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7542 - val_loss: 0.5908\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7480 - val_loss: 0.5980\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7472 - val_loss: 0.5726\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7502 - val_loss: 0.5785\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7477 - val_loss: 0.6179\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7468 - val_loss: 0.6240\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7521 - val_loss: 0.5938\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7459 - val_loss: 0.6033\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7391 - val_loss: 0.5616\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7342 - val_loss: 0.5819\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7319 - val_loss: 0.5797\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7391 - val_loss: 0.5884\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7277 - val_loss: 0.5798\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7364 - val_loss: 0.5720\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7309 - val_loss: 0.5668\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7339 - val_loss: 0.5687\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7274 - val_loss: 0.5671\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7260 - val_loss: 0.5629\n",
      "Epoch 28/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7220Restoring model weights from the end of the best epoch: 18.\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7216 - val_loss: 0.5711\n",
      "Epoch 28: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 1.2058 - val_loss: 0.7181\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.8450 - val_loss: 0.6628\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.8167 - val_loss: 0.6269\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8069 - val_loss: 0.6661\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7799 - val_loss: 0.5947\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7710 - val_loss: 0.5826\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7561 - val_loss: 0.5781\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7532 - val_loss: 0.6458\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7801 - val_loss: 0.5814\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7613 - val_loss: 0.6061\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7494 - val_loss: 0.5807\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7488 - val_loss: 0.5865\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7542 - val_loss: 0.6004\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7353 - val_loss: 0.5839\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7391 - val_loss: 0.5635\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7446 - val_loss: 0.5715\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7407 - val_loss: 0.6117\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7403 - val_loss: 0.5930\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7366 - val_loss: 0.5670\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7335 - val_loss: 0.5672\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7335 - val_loss: 0.5677\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7260 - val_loss: 0.5715\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7249 - val_loss: 0.5811\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7365 - val_loss: 0.5651\n",
      "Epoch 25/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7236Restoring model weights from the end of the best epoch: 15.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7234 - val_loss: 0.6348\n",
      "Epoch 25: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 90ms/step - loss: 1.1134 - val_loss: 0.7267\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.8480 - val_loss: 0.7180\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.8143 - val_loss: 0.6410\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7990 - val_loss: 0.6052\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7931 - val_loss: 0.5981\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7887 - val_loss: 0.6254\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7689 - val_loss: 0.5894\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7541 - val_loss: 0.6103\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7666 - val_loss: 0.5759\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7556 - val_loss: 0.5908\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7582 - val_loss: 0.6023\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7499 - val_loss: 0.6483\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7455 - val_loss: 0.5845\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7423 - val_loss: 0.6140\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7495 - val_loss: 0.5889\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7370 - val_loss: 0.5880\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7403 - val_loss: 0.5721\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7419 - val_loss: 0.5955\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7410 - val_loss: 0.5979\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7350 - val_loss: 0.5753\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7445 - val_loss: 0.5755\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7297 - val_loss: 0.5685\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7328 - val_loss: 0.5755\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7435 - val_loss: 0.5880\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7328 - val_loss: 0.5659\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7310 - val_loss: 0.6617\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7390 - val_loss: 0.5732\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7286 - val_loss: 0.5661\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7226 - val_loss: 0.6600\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7324 - val_loss: 0.5766\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7241 - val_loss: 0.5684\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7295 - val_loss: 0.5910\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7310 - val_loss: 0.5820\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7359 - val_loss: 0.5743\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7252 - val_loss: 0.5584\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7330 - val_loss: 0.5637\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7265 - val_loss: 0.5673\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7217 - val_loss: 0.5653\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7215 - val_loss: 0.6203\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7259 - val_loss: 0.5561\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7240 - val_loss: 0.5620\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7254 - val_loss: 0.5661\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7238 - val_loss: 0.5645\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7262 - val_loss: 0.5679\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7268 - val_loss: 0.5904\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7251 - val_loss: 0.5619\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7252 - val_loss: 0.5669\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7195 - val_loss: 0.5493\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7173 - val_loss: 0.5549\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7143 - val_loss: 0.5508\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7212 - val_loss: 0.5535\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7199 - val_loss: 0.5727\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7097 - val_loss: 0.5583\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7198 - val_loss: 0.6070\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7236 - val_loss: 0.5598\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7194 - val_loss: 0.5817\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7199 - val_loss: 0.5611\n",
      "Epoch 58/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7183Restoring model weights from the end of the best epoch: 48.\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7183 - val_loss: 0.5557\n",
      "Epoch 58: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 92ms/step - loss: 1.1078 - val_loss: 0.7570\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8630 - val_loss: 0.6421\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7950 - val_loss: 0.6410\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7899 - val_loss: 0.6179\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7922 - val_loss: 0.6378\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7708 - val_loss: 0.6201\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7575 - val_loss: 0.5922\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7550 - val_loss: 0.5811\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7537 - val_loss: 0.6396\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7488 - val_loss: 0.6094\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7469 - val_loss: 0.6310\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7528 - val_loss: 0.6034\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7389 - val_loss: 0.5965\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7355 - val_loss: 0.5819\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7387 - val_loss: 0.5782\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7300 - val_loss: 0.5987\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7466 - val_loss: 0.5880\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7479 - val_loss: 0.5691\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7347 - val_loss: 0.5732\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7322 - val_loss: 0.5556\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7312 - val_loss: 0.5699\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7384 - val_loss: 0.5738\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7336 - val_loss: 0.5803\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7370 - val_loss: 0.5903\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7431 - val_loss: 0.5842\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7417 - val_loss: 0.5538\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7284 - val_loss: 0.5832\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7342 - val_loss: 0.5702\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7279 - val_loss: 0.5628\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7223 - val_loss: 0.6686\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7392 - val_loss: 0.6264\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7359 - val_loss: 0.5786\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7248 - val_loss: 0.5596\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7317 - val_loss: 0.5559\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7290 - val_loss: 0.5682\n",
      "Epoch 36/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7338Restoring model weights from the end of the best epoch: 26.\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7343 - val_loss: 0.5729\n",
      "Epoch 36: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 1.1436 - val_loss: 0.7316\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8447 - val_loss: 0.6523\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.8038 - val_loss: 0.6747\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7829 - val_loss: 0.6668\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7878 - val_loss: 0.6129\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7713 - val_loss: 0.5882\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7694 - val_loss: 0.6567\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7613 - val_loss: 0.6359\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7724 - val_loss: 0.6296\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7749 - val_loss: 0.6041\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7423 - val_loss: 0.6324\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7532 - val_loss: 0.6094\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7523 - val_loss: 0.6096\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7402 - val_loss: 0.6028\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7376 - val_loss: 0.5785\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7380 - val_loss: 0.5635\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7421 - val_loss: 0.6143\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7500 - val_loss: 0.5871\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7475 - val_loss: 0.5789\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7446 - val_loss: 0.5779\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7317 - val_loss: 0.5814\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7364 - val_loss: 0.5629\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7359 - val_loss: 0.5666\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7329 - val_loss: 0.5567\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7288 - val_loss: 0.5809\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7270 - val_loss: 0.5664\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7319 - val_loss: 0.5662\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7270 - val_loss: 0.6115\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7247 - val_loss: 0.5770\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7316 - val_loss: 0.5633\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7236 - val_loss: 0.5621\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7269 - val_loss: 0.5589\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7256 - val_loss: 0.5589\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7373 - val_loss: 0.5561\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7234 - val_loss: 0.5783\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7311 - val_loss: 0.5534\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7280 - val_loss: 0.5814\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7285 - val_loss: 0.5797\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7293 - val_loss: 0.6016\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7317 - val_loss: 0.5618\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7206 - val_loss: 0.5593\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7231 - val_loss: 0.5560\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7185 - val_loss: 0.5554\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7262 - val_loss: 0.5722\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7252 - val_loss: 0.5668\n",
      "Epoch 46/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7240Restoring model weights from the end of the best epoch: 36.\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7246 - val_loss: 0.5574\n",
      "Epoch 46: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 1.1680 - val_loss: 0.7741\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.8429 - val_loss: 0.6231\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8051 - val_loss: 0.6372\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.8021 - val_loss: 0.6196\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7788 - val_loss: 0.6526\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7769 - val_loss: 0.5899\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7616 - val_loss: 0.5955\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7603 - val_loss: 0.6019\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7530 - val_loss: 0.5825\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7503 - val_loss: 0.5946\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7527 - val_loss: 0.5929\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7449 - val_loss: 0.6167\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7505 - val_loss: 0.6024\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7403 - val_loss: 0.5936\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7495 - val_loss: 0.5720\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7485 - val_loss: 0.6012\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7486 - val_loss: 0.5847\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7386 - val_loss: 0.5884\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7432 - val_loss: 0.5817\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7340 - val_loss: 0.5988\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7331 - val_loss: 0.5626\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7296 - val_loss: 0.5809\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7325 - val_loss: 0.5700\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7337 - val_loss: 0.5753\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7267 - val_loss: 0.5821\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7374 - val_loss: 0.5671\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7419 - val_loss: 0.5743\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7300 - val_loss: 0.5775\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7333 - val_loss: 0.5627\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7295 - val_loss: 0.5850\n",
      "Epoch 31/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7311Restoring model weights from the end of the best epoch: 21.\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7319 - val_loss: 0.5660\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 6238834.5000 - val_loss: 899529.8750\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 453337.6250 - val_loss: 190528.2656\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 358234.1875 - val_loss: 588150.2500\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 508439.7500 - val_loss: 368132.9375\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 365514.8438 - val_loss: 77489.1562\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 414570.4375 - val_loss: 258609.2500\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 239845.5781 - val_loss: 138845.6406\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 523415.1875 - val_loss: 339204.7500\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 233149.3906 - val_loss: 277720.8750\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 347031.7812 - val_loss: 69428.9609\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 198428.4375 - val_loss: 123712.1406\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 192530.8906 - val_loss: 184047.5625\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 174634.8750 - val_loss: 197761.8125\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 150176.8594 - val_loss: 237689.5000\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 260328.8125 - val_loss: 151920.5625\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 241935.9219 - val_loss: 157245.6875\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 145949.0312 - val_loss: 131685.6094\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 152914.4062 - val_loss: 213455.6719\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 174496.3594 - val_loss: 90642.4297\n",
      "Epoch 20/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 250576.4219Restoring model weights from the end of the best epoch: 10.\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 250529.6406 - val_loss: 222127.8125\n",
      "Epoch 20: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 4311777.0000 - val_loss: 684200.1875\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 526522.3125 - val_loss: 523955.2188\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 462278.0000 - val_loss: 265392.2812\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 342508.5625 - val_loss: 622970.8125\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 343611.9375 - val_loss: 296725.6562\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 251728.4688 - val_loss: 520726.8750\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 451980.2812 - val_loss: 252867.3594\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 356981.4375 - val_loss: 469186.1250\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 293145.0625 - val_loss: 138491.9219\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 292531.5312 - val_loss: 193512.0469\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 231484.0000 - val_loss: 406637.4062\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 173714.9844 - val_loss: 306766.9688\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 171566.6250 - val_loss: 354474.0000\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 174091.7656 - val_loss: 166494.9219\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 187861.6875 - val_loss: 368778.5625\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 215034.2969 - val_loss: 300060.5000\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 181029.1250 - val_loss: 201511.7031\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 152270.8438 - val_loss: 79664.7344\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 163761.3438 - val_loss: 26083.2285\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 211577.6562 - val_loss: 146140.8750\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 109150.4922 - val_loss: 459467.1250\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 212693.9375 - val_loss: 133539.9688\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 153006.4844 - val_loss: 80756.5625\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 203445.2656 - val_loss: 74261.0859\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 155519.4844 - val_loss: 19918.5586\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 130758.5469 - val_loss: 130798.3594\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 113042.5859 - val_loss: 171100.2969\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 114025.1406 - val_loss: 155775.3750\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 116036.2500 - val_loss: 50932.9180\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 168979.5469 - val_loss: 92934.9766\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 210980.6094 - val_loss: 110536.3750\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 118305.8125 - val_loss: 204412.7500\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 124082.3984 - val_loss: 308290.0938\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 292508.1250 - val_loss: 154784.2031\n",
      "Epoch 35/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 128691.8594Restoring model weights from the end of the best epoch: 25.\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 128532.1875 - val_loss: 115854.2500\n",
      "Epoch 35: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 95ms/step - loss: 5074394.0000 - val_loss: 389112.2812\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 506795.3125 - val_loss: 399777.3750\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 353348.9688 - val_loss: 300135.5000\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 330498.2188 - val_loss: 508805.5000\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 455793.3125 - val_loss: 186483.5156\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 576376.8750 - val_loss: 470115.3438\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 335202.0312 - val_loss: 391856.7500\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 271835.1875 - val_loss: 179933.1562\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 429860.0938 - val_loss: 298492.1250\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 235798.2188 - val_loss: 87514.1562\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 219342.2656 - val_loss: 695563.6250\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 223741.5156 - val_loss: 89408.3516\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 280541.6250 - val_loss: 137197.1719\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 219662.4531 - val_loss: 83853.5312\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 139065.0938 - val_loss: 147023.8750\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 195657.7969 - val_loss: 142441.3125\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 204157.4531 - val_loss: 519432.5625\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 190007.6094 - val_loss: 285074.9375\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 157303.5469 - val_loss: 225806.3594\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 140822.5781 - val_loss: 81317.4688\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 237782.9844 - val_loss: 189646.0625\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 181581.1094 - val_loss: 366320.4062\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 182904.9844 - val_loss: 212129.1406\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 111944.5703 - val_loss: 308498.6250\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 146215.7500 - val_loss: 96505.6406\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 119434.6875 - val_loss: 379728.9375\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 211027.4688 - val_loss: 36245.5938\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 98794.2266 - val_loss: 41271.6406\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 208543.1719 - val_loss: 229196.2188\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 135113.8438 - val_loss: 285707.0625\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 163913.3594 - val_loss: 46132.8477\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 162575.2031 - val_loss: 238240.1094\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 149726.5781 - val_loss: 212276.6875\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 185964.3594 - val_loss: 109487.2344\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 78270.8516 - val_loss: 45799.5039\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 146119.9375 - val_loss: 214666.2656\n",
      "Epoch 37/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 171364.0156Restoring model weights from the end of the best epoch: 27.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 171502.6719 - val_loss: 159597.7500\n",
      "Epoch 37: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 5432080.5000 - val_loss: 1090187.2500\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 732822.2500 - val_loss: 523895.6250\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 638978.0625 - val_loss: 472384.3438\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 670091.8750 - val_loss: 1707897.1250\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 434786.9375 - val_loss: 243800.8906\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 349430.6875 - val_loss: 313754.4375\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 477151.5625 - val_loss: 777682.5625\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 457015.5000 - val_loss: 198426.1562\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 314503.5625 - val_loss: 486323.9375\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 271458.0312 - val_loss: 435574.0312\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 310384.7812 - val_loss: 181801.3906\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 193546.3594 - val_loss: 191694.6094\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 259259.6250 - val_loss: 336633.0000\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 304397.9062 - val_loss: 499150.0625\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 342635.8438 - val_loss: 145707.6562\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 183822.6406 - val_loss: 120916.5547\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 176448.3750 - val_loss: 174883.9375\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 287569.6875 - val_loss: 83426.2266\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 175683.6250 - val_loss: 89012.5938\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 290660.2500 - val_loss: 254378.1094\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 312495.8125 - val_loss: 265082.3750\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 151695.9375 - val_loss: 217330.0625\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 206222.4375 - val_loss: 58899.4023\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 174813.4844 - val_loss: 392608.4688\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 140227.8750 - val_loss: 157338.3125\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 185919.3125 - val_loss: 41575.9414\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 173004.5625 - val_loss: 193345.0469\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 132286.9531 - val_loss: 195923.7812\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 118010.5078 - val_loss: 254562.7188\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 233261.0000 - val_loss: 188884.8906\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 153353.5312 - val_loss: 76895.0703\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 151817.5781 - val_loss: 44240.4688\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 217333.4062 - val_loss: 978479.0000\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 212583.1406 - val_loss: 158496.1250\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 108064.2969 - val_loss: 590745.6250\n",
      "Epoch 36/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 246677.7812Restoring model weights from the end of the best epoch: 26.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 246434.9062 - val_loss: 397274.8750\n",
      "Epoch 36: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 3972326.7500 - val_loss: 169347.4844\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 1133570.6250 - val_loss: 738316.7500\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 469978.1250 - val_loss: 764268.3125\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 742314.0000 - val_loss: 268139.4688\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 364291.3750 - val_loss: 82600.8359\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 390884.7812 - val_loss: 339402.2188\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 362592.0312 - val_loss: 325209.9375\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 300911.8438 - val_loss: 222522.6250\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 300999.1250 - val_loss: 317737.8125\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 212197.2969 - val_loss: 493948.3438\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 388510.5312 - val_loss: 371110.8750\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 265483.7188 - val_loss: 185715.7344\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 250664.7656 - val_loss: 411888.3750\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 236416.4375 - val_loss: 578448.6250\n",
      "Epoch 15/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 233595.4219Restoring model weights from the end of the best epoch: 5.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 233420.2031 - val_loss: 384570.9688\n",
      "Epoch 15: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 4788456.0000 - val_loss: 250696.1094\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 753713.3750 - val_loss: 1168921.8750\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 879602.1875 - val_loss: 424286.6875\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 524056.7500 - val_loss: 298575.0312\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 358353.4062 - val_loss: 878214.4375\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 468001.8438 - val_loss: 1324175.7500\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 535441.6875 - val_loss: 219914.1406\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 333290.2188 - val_loss: 261065.9219\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 278262.6250 - val_loss: 298546.9688\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 440525.8438 - val_loss: 532384.5625\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 331292.4062 - val_loss: 242835.7188\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 180956.6719 - val_loss: 508506.3750\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 317457.0000 - val_loss: 545651.0000\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 352625.1250 - val_loss: 553297.2500\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 353231.7812 - val_loss: 172232.4688\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 198446.0938 - val_loss: 100512.9219\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 159021.5938 - val_loss: 306040.6250\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 297658.9062 - val_loss: 117873.3359\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 161476.3281 - val_loss: 208190.7500\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 228503.4531 - val_loss: 484673.8750\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 155925.7969 - val_loss: 266133.1875\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 208019.3438 - val_loss: 419972.8125\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 182809.0469 - val_loss: 364348.3750\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 137463.5625 - val_loss: 208832.4062\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 164659.7656 - val_loss: 116513.3594\n",
      "Epoch 26/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 133548.0625Restoring model weights from the end of the best epoch: 16.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 133866.1406 - val_loss: 132817.4844\n",
      "Epoch 26: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 102ms/step - loss: 3435623.5000 - val_loss: 1509259.2500\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 624227.3750 - val_loss: 610685.6875\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 504004.3125 - val_loss: 289049.2188\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 594547.9375 - val_loss: 589830.1250\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 480537.4688 - val_loss: 687268.8125\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 325395.5938 - val_loss: 585948.3750\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 380350.0938 - val_loss: 314600.4375\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 413152.3438 - val_loss: 301715.3125\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 290033.5938 - val_loss: 195760.0312\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 423594.3438 - val_loss: 592339.1875\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 383917.1875 - val_loss: 149754.7500\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 235384.0469 - val_loss: 344290.1250\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 164164.8594 - val_loss: 100046.3828\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 230832.9688 - val_loss: 80972.8281\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 213031.0781 - val_loss: 176925.6562\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 201540.0156 - val_loss: 189118.4219\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 231632.1719 - val_loss: 50905.7969\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 168619.3906 - val_loss: 269279.7188\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 225080.7969 - val_loss: 270008.6562\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 149713.5469 - val_loss: 69804.7891\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 184975.5156 - val_loss: 244066.5312\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 213661.0156 - val_loss: 395184.3438\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 285176.1562 - val_loss: 113534.3984\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 98703.7578 - val_loss: 126203.5391\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 184630.4531 - val_loss: 102557.7188\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 117946.1641 - val_loss: 96078.1719\n",
      "Epoch 27/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 193554.8125Restoring model weights from the end of the best epoch: 17.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 193534.2188 - val_loss: 137402.2188\n",
      "Epoch 27: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 5236438.0000 - val_loss: 1643458.2500\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 728044.5625 - val_loss: 844314.1250\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 551988.3750 - val_loss: 845618.1250\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 484085.2500 - val_loss: 608979.3125\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 308958.5312 - val_loss: 85443.7109\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 311799.2812 - val_loss: 132478.3594\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 265043.9375 - val_loss: 563652.2500\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 292122.7500 - val_loss: 328640.6875\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 248180.5469 - val_loss: 302639.1250\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 312660.8438 - val_loss: 401001.8750\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 250403.2656 - val_loss: 182552.4062\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 274475.9062 - val_loss: 368036.6562\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 247218.8906 - val_loss: 519188.9375\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 293792.0312 - val_loss: 142067.3125\n",
      "Epoch 15/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 310970.5938Restoring model weights from the end of the best epoch: 5.\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 310753.9375 - val_loss: 258641.7812\n",
      "Epoch 15: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 3191151.7500 - val_loss: 346795.7500\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 720677.1250 - val_loss: 1905916.8750\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 747328.2500 - val_loss: 879727.2500\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 593987.0000 - val_loss: 606914.7500\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 319446.7500 - val_loss: 116423.8594\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 444358.8438 - val_loss: 511854.0000\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 277359.2188 - val_loss: 657880.1875\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 438260.8125 - val_loss: 245091.6875\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 208125.4844 - val_loss: 253888.0625\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 208736.0000 - val_loss: 290894.9375\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 287960.7500 - val_loss: 306780.6250\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 194122.1562 - val_loss: 571100.4375\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 276152.4688 - val_loss: 252676.3906\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 242237.0312 - val_loss: 88161.0547\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 197081.6719 - val_loss: 41908.3047\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 322949.5625 - val_loss: 208168.4062\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 308457.3438 - val_loss: 69811.2344\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 209352.8594 - val_loss: 364175.7188\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 175611.1719 - val_loss: 136776.5781\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 150778.8906 - val_loss: 288093.3438\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125506.7891 - val_loss: 62622.3672\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 141343.0625 - val_loss: 305337.1250\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 147554.3906 - val_loss: 515590.8438\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 166332.2812 - val_loss: 177626.6562\n",
      "Epoch 25/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 119267.5859Restoring model weights from the end of the best epoch: 15.\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 119095.7109 - val_loss: 222790.8125\n",
      "Epoch 25: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 6500445.5000 - val_loss: 1589403.5000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 540537.6250 - val_loss: 317779.6875\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 678033.6250 - val_loss: 356654.3750\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 354443.6875 - val_loss: 253476.1875\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 426754.8438 - val_loss: 390804.7188\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 267030.1250 - val_loss: 302600.8750\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 615993.5625 - val_loss: 390013.6250\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 349416.8750 - val_loss: 207699.4375\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 298598.1562 - val_loss: 457260.8438\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 210266.0938 - val_loss: 220437.0781\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 257103.7188 - val_loss: 155569.0312\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 235068.0312 - val_loss: 235787.0625\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 294063.3438 - val_loss: 240905.7812\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 224344.7969 - val_loss: 137796.7031\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 187525.7031 - val_loss: 361930.6250\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 186000.5156 - val_loss: 272065.3750\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 184818.3906 - val_loss: 115806.1641\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 153665.2969 - val_loss: 179740.8281\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 263731.5312 - val_loss: 404807.0625\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 229211.0625 - val_loss: 63355.9219\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 98517.7969 - val_loss: 163003.6250\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 215483.5000 - val_loss: 96512.2656\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 169759.3594 - val_loss: 133723.5625\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 166942.9688 - val_loss: 90826.3594\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 239947.5000 - val_loss: 180140.3438\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 106252.9844 - val_loss: 129489.2891\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 167587.4375 - val_loss: 273936.0625\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 133338.8906 - val_loss: 223117.9375\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 181786.5156 - val_loss: 144610.4688\n",
      "Epoch 30/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 184794.4375Restoring model weights from the end of the best epoch: 20.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 184869.9219 - val_loss: 201123.7031\n",
      "Epoch 30: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 187.3351 - val_loss: 141.5981\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 132.5300 - val_loss: 135.7451\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 130.7748 - val_loss: 135.4184\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 128.2771 - val_loss: 132.0625\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 127.3756 - val_loss: 131.8313\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 127.0899 - val_loss: 130.9017\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.2731 - val_loss: 129.9577\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.8990 - val_loss: 131.7148\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.9182 - val_loss: 130.8233\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 124.7402 - val_loss: 130.6747\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.6015 - val_loss: 130.1465\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 124.5903 - val_loss: 129.4501\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.4348 - val_loss: 128.8424\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.7525 - val_loss: 128.3547\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.2157 - val_loss: 128.6940\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 124.0647 - val_loss: 131.3447\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 123.4478 - val_loss: 127.7731\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.6329 - val_loss: 127.9321\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.3268 - val_loss: 129.0396\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.8245 - val_loss: 128.3731\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.1236 - val_loss: 127.6483\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.7168 - val_loss: 126.7259\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 122.1012 - val_loss: 126.2603\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 122.3033 - val_loss: 129.2392\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 122.5219 - val_loss: 128.2166\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 121.7756 - val_loss: 128.7043\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.0069 - val_loss: 126.5400\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.8515 - val_loss: 128.1771\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 121.5441 - val_loss: 127.2222\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.1779 - val_loss: 126.4227\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.6140 - val_loss: 126.6616\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 121.8139 - val_loss: 125.6203\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 121.0518 - val_loss: 125.3206\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.4820 - val_loss: 125.6001\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.8471 - val_loss: 127.1511\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.7846 - val_loss: 127.3228\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.6420 - val_loss: 125.9797\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 120.7127 - val_loss: 126.2611\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.6097 - val_loss: 125.6758\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.6282 - val_loss: 125.4873\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.5847 - val_loss: 125.1256\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.0651 - val_loss: 124.9543\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.9743 - val_loss: 126.3753\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.0529 - val_loss: 125.7223\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.0493 - val_loss: 125.4873\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.6303 - val_loss: 125.9502\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.6570 - val_loss: 126.1883\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.9333 - val_loss: 125.1047\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 120.9508 - val_loss: 125.8521\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.9190 - val_loss: 127.1490\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 120.3333 - val_loss: 125.2934\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 120.4552 - val_loss: 124.6199\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.4275 - val_loss: 125.3252\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 120.1797 - val_loss: 124.9056\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 120.4061 - val_loss: 127.1953\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 120.3052 - val_loss: 125.6168\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.3453 - val_loss: 124.8390\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.4409 - val_loss: 124.6790\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 119.7240 - val_loss: 124.4218\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 120.2459 - val_loss: 124.9507\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.4145 - val_loss: 124.4488\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 120.3677 - val_loss: 125.7487\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.3830 - val_loss: 125.7491\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 119.9672 - val_loss: 125.0548\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 119.8374 - val_loss: 125.6479\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 119.9649 - val_loss: 124.7274\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 120.3273 - val_loss: 124.7928\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.0298 - val_loss: 125.7591\n",
      "Epoch 69/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 120.1608Restoring model weights from the end of the best epoch: 59.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.1560 - val_loss: 128.9141\n",
      "Epoch 69: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 99ms/step - loss: 137.8145 - val_loss: 138.7641\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 134.7043 - val_loss: 138.5914\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 130.9214 - val_loss: 133.8642\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 128.8396 - val_loss: 131.9868\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 127.0629 - val_loss: 132.1384\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 126.7760 - val_loss: 132.5701\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.7965 - val_loss: 132.7012\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.0742 - val_loss: 129.7378\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.1976 - val_loss: 130.8546\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.4630 - val_loss: 128.3910\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.0508 - val_loss: 128.4614\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 123.5062 - val_loss: 128.1728\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.3535 - val_loss: 127.3654\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.3496 - val_loss: 129.9627\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.3087 - val_loss: 128.6542\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.5218 - val_loss: 128.0285\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 122.2789 - val_loss: 127.5117\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.2262 - val_loss: 126.0596\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.8242 - val_loss: 129.7910\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 122.0480 - val_loss: 126.7867\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.6455 - val_loss: 126.1962\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 122.4819 - val_loss: 128.0355\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.5761 - val_loss: 127.8266\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.5879 - val_loss: 126.1261\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.6675 - val_loss: 126.1847\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.4132 - val_loss: 128.1662\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.3135 - val_loss: 126.6175\n",
      "Epoch 28/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 121.0246Restoring model weights from the end of the best epoch: 18.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.0443 - val_loss: 126.2913\n",
      "Epoch 28: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 136.9210 - val_loss: 139.0970\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 131.2728 - val_loss: 135.0824\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 128.6450 - val_loss: 133.8589\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 128.3273 - val_loss: 133.9722\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 126.9813 - val_loss: 133.2317\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 126.2407 - val_loss: 131.5564\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 127.2768 - val_loss: 130.5004\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 125.6912 - val_loss: 129.7921\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 126.0277 - val_loss: 131.7213\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 124.8541 - val_loss: 130.7130\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 124.4824 - val_loss: 129.5268\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 124.0074 - val_loss: 129.1521\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 124.2513 - val_loss: 128.5837\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 123.0207 - val_loss: 127.8355\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.2051 - val_loss: 127.9700\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 123.1746 - val_loss: 126.9731\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 123.1164 - val_loss: 127.1734\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 123.6913 - val_loss: 127.8336\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.6728 - val_loss: 127.7708\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.2580 - val_loss: 127.1420\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.1411 - val_loss: 128.0594\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 121.3815 - val_loss: 125.2922\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.2950 - val_loss: 125.7724\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.3008 - val_loss: 129.3916\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 122.1210 - val_loss: 125.6800\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 121.2777 - val_loss: 127.2012\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.3812 - val_loss: 126.1510\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.8973 - val_loss: 126.6613\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.9854 - val_loss: 127.0220\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.0541 - val_loss: 125.3217\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.1545 - val_loss: 125.8818\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 120.4537Restoring model weights from the end of the best epoch: 22.\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 120.4537 - val_loss: 125.3372\n",
      "Epoch 32: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 18s 169ms/step - loss: 138.0266 - val_loss: 136.5630\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 11s 146ms/step - loss: 132.1688 - val_loss: 136.4350\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 11s 144ms/step - loss: 130.4072 - val_loss: 133.0251\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 11s 146ms/step - loss: 128.4105 - val_loss: 133.6158\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 11s 147ms/step - loss: 126.7940 - val_loss: 131.4137\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 11s 146ms/step - loss: 126.9378 - val_loss: 130.8310\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 11s 148ms/step - loss: 127.1861 - val_loss: 130.9322\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 11s 147ms/step - loss: 125.3729 - val_loss: 131.9373\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 11s 146ms/step - loss: 125.2719 - val_loss: 129.1900\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 11s 146ms/step - loss: 124.2952 - val_loss: 129.4205\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 11s 148ms/step - loss: 127.1114 - val_loss: 129.6143\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 11s 146ms/step - loss: 123.8208 - val_loss: 129.6363\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 11s 146ms/step - loss: 123.9207 - val_loss: 129.2728\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 11s 149ms/step - loss: 124.0633 - val_loss: 127.9085\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 13s 181ms/step - loss: 122.9224 - val_loss: 129.8612\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 34s 463ms/step - loss: 123.4034 - val_loss: 133.4867\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 126.7037 - val_loss: 130.7721\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.2368 - val_loss: 130.0477\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.4991 - val_loss: 130.6969\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.9994 - val_loss: 126.9377\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.0064 - val_loss: 126.3304\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.2330 - val_loss: 126.6517\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.1967 - val_loss: 126.6615\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.8963 - val_loss: 127.3501\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.7700 - val_loss: 128.0587\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.3394 - val_loss: 126.9791\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.3726 - val_loss: 127.2648\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.3157 - val_loss: 131.5274\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 124.9063 - val_loss: 128.4684\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 122.1478 - val_loss: 126.7223\n",
      "Epoch 31/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 121.0401Restoring model weights from the end of the best epoch: 21.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.0790 - val_loss: 126.4790\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 139.8748 - val_loss: 137.8031\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 131.6702 - val_loss: 136.0343\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 129.5351 - val_loss: 133.2843\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 127.6009 - val_loss: 132.1955\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 126.9191 - val_loss: 133.0484\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 126.6069 - val_loss: 131.6807\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.5194 - val_loss: 130.7888\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.8439 - val_loss: 129.3645\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.0445 - val_loss: 129.0114\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 124.6510 - val_loss: 129.1063\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.5169 - val_loss: 129.7624\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.6355 - val_loss: 128.5008\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 125.1233 - val_loss: 131.6969\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.2442 - val_loss: 128.8241\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 123.5940 - val_loss: 129.6580\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.8232 - val_loss: 126.9904\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.0239 - val_loss: 126.5422\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.5336 - val_loss: 128.9136\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.7963 - val_loss: 129.5620\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 123.6545 - val_loss: 132.1982\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 123.9346 - val_loss: 128.6341\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.5063 - val_loss: 126.2936\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.0066 - val_loss: 128.2259\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.8194 - val_loss: 127.4129\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.3495 - val_loss: 128.0706\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.2251 - val_loss: 128.7516\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.8255 - val_loss: 128.5289\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.7947 - val_loss: 128.8556\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.7592 - val_loss: 125.2339\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.6438 - val_loss: 127.1380\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.0570 - val_loss: 125.8042\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.5064 - val_loss: 126.7915\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.9973 - val_loss: 127.4435\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.0864 - val_loss: 125.3627\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.7050 - val_loss: 126.0812\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.7467 - val_loss: 126.6252\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.0444 - val_loss: 125.8518\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.2010 - val_loss: 125.6327\n",
      "Epoch 39/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 121.0508Restoring model weights from the end of the best epoch: 29.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.0473 - val_loss: 126.7351\n",
      "Epoch 39: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 143.9251 - val_loss: 138.5144\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 132.9884 - val_loss: 135.6648\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 129.0091 - val_loss: 135.5624\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 127.9202 - val_loss: 133.3388\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 127.0119 - val_loss: 131.2430\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 126.5239 - val_loss: 132.0335\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 125.5668 - val_loss: 131.2083\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 127.6052 - val_loss: 132.4445\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 125.9770 - val_loss: 129.7359\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 125.1444 - val_loss: 131.8232\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 125.4378 - val_loss: 129.6116\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.2620 - val_loss: 129.1782\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 124.0678 - val_loss: 129.0558\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 124.1756 - val_loss: 130.0159\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 124.5604 - val_loss: 132.0164\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 124.1862 - val_loss: 129.1104\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 123.0527 - val_loss: 128.4743\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.7530 - val_loss: 126.6512\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.1935 - val_loss: 128.7819\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 122.1342 - val_loss: 128.0176\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 121.8369 - val_loss: 129.1207\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 121.8669 - val_loss: 126.0981\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.9806 - val_loss: 127.1054\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.6052 - val_loss: 127.1334\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.6114 - val_loss: 126.8064\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 122.2079 - val_loss: 126.2456\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.8059 - val_loss: 124.9189\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.6618 - val_loss: 127.1036\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.6346 - val_loss: 126.7011\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 121.4039 - val_loss: 126.0643\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.3032 - val_loss: 126.4908\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 120.9597 - val_loss: 125.9424\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 121.2153 - val_loss: 126.4881\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 120.7429 - val_loss: 125.6636\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.6940 - val_loss: 126.8926\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 120.9908 - val_loss: 125.9376\n",
      "Epoch 37/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 120.9633Restoring model weights from the end of the best epoch: 27.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.9410 - val_loss: 125.2616\n",
      "Epoch 37: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 110ms/step - loss: 137.6010 - val_loss: 139.3741\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 131.4052 - val_loss: 137.2202\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 130.5472 - val_loss: 135.1855\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 128.1558 - val_loss: 133.3498\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 126.8783 - val_loss: 132.6079\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 127.0769 - val_loss: 131.8683\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 125.8881 - val_loss: 131.7838\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 125.6298 - val_loss: 128.8282\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 124.7728 - val_loss: 129.9345\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 126.2829 - val_loss: 129.6389\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 124.4182 - val_loss: 128.3365\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 124.2876 - val_loss: 128.2027\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 123.7212 - val_loss: 131.5939\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 124.4579 - val_loss: 129.5570\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 123.5668 - val_loss: 128.8100\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 122.7514 - val_loss: 128.3987\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 123.7553 - val_loss: 128.4193\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 123.4519 - val_loss: 127.7406\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 122.5193 - val_loss: 128.1164\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 122.4107 - val_loss: 126.9720\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 122.1001 - val_loss: 126.6061\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 122.0288 - val_loss: 128.2704\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 121.5857 - val_loss: 126.8098\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 122.4538 - val_loss: 128.4267\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 122.1558 - val_loss: 126.4384\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 120.9209 - val_loss: 129.7858\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.2617 - val_loss: 126.5264\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 122.3009 - val_loss: 127.6517\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 121.1163 - val_loss: 126.3188\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 121.1753 - val_loss: 125.7092\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 120.7937 - val_loss: 126.2427\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 121.1439 - val_loss: 128.2422\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 121.0330 - val_loss: 127.1349\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 120.9573 - val_loss: 127.7536\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 121.0468 - val_loss: 125.6072\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 120.9379 - val_loss: 126.2586\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 120.8123 - val_loss: 126.2569\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 120.7780 - val_loss: 126.8065\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 120.3424 - val_loss: 126.0865\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 120.8237 - val_loss: 125.9212\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 120.2984 - val_loss: 125.4229\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 120.6001 - val_loss: 126.7274\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 120.4763 - val_loss: 125.6480\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 120.5962 - val_loss: 126.3900\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 121.0719 - val_loss: 124.5158\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 120.3810 - val_loss: 124.9398\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 120.3762 - val_loss: 125.0769\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 120.6665 - val_loss: 127.1456\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 120.2976 - val_loss: 125.1428\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 120.0454 - val_loss: 125.6894\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 121.2659 - val_loss: 126.2676\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 120.4407 - val_loss: 125.4070\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 120.0184 - val_loss: 125.6018\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 120.2141 - val_loss: 124.2459\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 120.1813 - val_loss: 124.6375\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 120.0085 - val_loss: 124.3323\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 120.1291 - val_loss: 126.1132\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 120.2527 - val_loss: 126.5068\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 120.0071 - val_loss: 126.1714\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 120.1455 - val_loss: 124.7798\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 120.1440 - val_loss: 124.8669\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 119.7171 - val_loss: 125.4611\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 119.8462 - val_loss: 124.9987\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 119.8456Restoring model weights from the end of the best epoch: 54.\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 119.8456 - val_loss: 126.9311\n",
      "Epoch 64: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 108ms/step - loss: 137.4346 - val_loss: 138.8653\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 131.8028 - val_loss: 134.9499\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 130.1354 - val_loss: 132.1171\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 127.7313 - val_loss: 132.6742\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 126.5998 - val_loss: 133.2972\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 125.5635 - val_loss: 131.6436\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 125.9040 - val_loss: 130.4166\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 126.1166 - val_loss: 129.7499\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 124.9174 - val_loss: 129.8445\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 124.8344 - val_loss: 129.4544\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 123.9826 - val_loss: 129.5645\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 124.5515 - val_loss: 128.7240\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 125.2694 - val_loss: 128.5322\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.5541 - val_loss: 128.0266\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 123.5844 - val_loss: 129.3441\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 124.0612 - val_loss: 127.3283\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 122.1572 - val_loss: 129.3791\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 121.7862 - val_loss: 131.5302\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.4175 - val_loss: 126.7769\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 121.5646 - val_loss: 127.1064\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 123.4462 - val_loss: 127.6229\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 123.2308 - val_loss: 126.8107\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 121.8429 - val_loss: 127.6787\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 121.3806 - val_loss: 125.4113\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 122.0146 - val_loss: 126.5191\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 121.2104 - val_loss: 125.7696\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 120.9335 - val_loss: 128.1239\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.9879 - val_loss: 126.0698\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 121.7040 - val_loss: 126.6522\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 122.2541 - val_loss: 126.9108\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 121.5951 - val_loss: 125.3611\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 121.6175 - val_loss: 126.5294\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.1270 - val_loss: 125.3004\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.1728 - val_loss: 128.4469\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.6599 - val_loss: 124.9178\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.0105 - val_loss: 126.8837\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.8691 - val_loss: 124.6620\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.9276 - val_loss: 126.4983\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.9819 - val_loss: 126.0612\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.7046 - val_loss: 127.7995\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.6787 - val_loss: 126.7821\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.1650 - val_loss: 125.9950\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.0201 - val_loss: 126.3047\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.8026 - val_loss: 125.1155\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.1222 - val_loss: 125.9603\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.4227 - val_loss: 127.5696\n",
      "Epoch 47/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 120.5429Restoring model weights from the end of the best epoch: 37.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.5551 - val_loss: 126.4723\n",
      "Epoch 47: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 139.6157 - val_loss: 140.2938\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 130.6605 - val_loss: 133.9151\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 130.6261 - val_loss: 135.9847\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 128.0520 - val_loss: 131.5039\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 127.1819 - val_loss: 131.6919\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 126.3493 - val_loss: 130.1394\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 126.5177 - val_loss: 130.0824\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 125.8652 - val_loss: 130.8513\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 125.5964 - val_loss: 131.2436\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 125.0240 - val_loss: 131.0740\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.6586 - val_loss: 131.4863\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.2595 - val_loss: 129.4713\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.9346 - val_loss: 130.7569\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.9779 - val_loss: 128.0126\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 124.9012 - val_loss: 130.5725\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 123.6633 - val_loss: 127.7802\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 123.2612 - val_loss: 128.4215\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.0509 - val_loss: 128.9816\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.6332 - val_loss: 129.5095\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 126.0158 - val_loss: 128.4281\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.2708 - val_loss: 127.5483\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 122.5275 - val_loss: 128.5757\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 122.7483 - val_loss: 129.5946\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.5549 - val_loss: 126.8456\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.1360 - val_loss: 127.7289\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.7334 - val_loss: 128.1333\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 121.5930 - val_loss: 126.2968\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.1932 - val_loss: 126.3364\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.1416 - val_loss: 130.2298\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.5708 - val_loss: 125.5727\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.9889 - val_loss: 126.3010\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.2299 - val_loss: 126.3458\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.9489 - val_loss: 129.0844\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.7794 - val_loss: 126.3916\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.6866 - val_loss: 126.8799\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.5820 - val_loss: 126.7443\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 121.7964 - val_loss: 126.2974\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.3535 - val_loss: 127.1238\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 120.3838 - val_loss: 126.4657\n",
      "Epoch 40/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 121.5107Restoring model weights from the end of the best epoch: 30.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.5082 - val_loss: 126.1243\n",
      "Epoch 40: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 139.4754 - val_loss: 138.5390\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 132.3912 - val_loss: 134.9874\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 130.5683 - val_loss: 134.6850\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 128.3006 - val_loss: 132.8299\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 127.0542 - val_loss: 131.3598\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 126.5219 - val_loss: 132.4237\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 126.0953 - val_loss: 129.9589\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.6955 - val_loss: 129.8602\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.8056 - val_loss: 129.7359\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.4381 - val_loss: 130.4762\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 126.0178 - val_loss: 132.2393\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.6545 - val_loss: 130.8307\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 124.3934 - val_loss: 128.0881\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 124.0740 - val_loss: 129.0657\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.5246 - val_loss: 128.4671\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.5818 - val_loss: 128.9933\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.4567 - val_loss: 131.6486\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.4971 - val_loss: 127.2802\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.5260 - val_loss: 127.0232\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.5181 - val_loss: 128.3573\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.1378 - val_loss: 126.8639\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.2128 - val_loss: 126.6339\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.2701 - val_loss: 129.4770\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.3135 - val_loss: 126.1988\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.5450 - val_loss: 130.1625\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.8367 - val_loss: 128.4682\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.7164 - val_loss: 126.5842\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.2487 - val_loss: 127.8999\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.5401 - val_loss: 126.9753\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.9629 - val_loss: 125.8484\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.1765 - val_loss: 125.8288\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.0152 - val_loss: 126.9925\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.2831 - val_loss: 126.4008\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.9397 - val_loss: 126.8694\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.2927 - val_loss: 125.3456\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.5314 - val_loss: 126.7253\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.2847 - val_loss: 128.2298\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.6334 - val_loss: 127.2577\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.7722 - val_loss: 126.0411\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.0850 - val_loss: 126.1031\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.8107 - val_loss: 127.4948\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.4376 - val_loss: 126.3278\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.9396 - val_loss: 126.5981\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.6747 - val_loss: 126.6354\n",
      "Epoch 45/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 121.1532Restoring model weights from the end of the best epoch: 35.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.1866 - val_loss: 126.8323\n",
      "Epoch 45: early stopping\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),2000,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',2000,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),2000,10,8,0.001)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "#mae_models = train_bagging_models(model_num, 'mae',2000,10,8,0.001)\n",
    "#mse_models = train_bagging_models(model_num, 'mse',2000,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea5d27a-1d7b-42a9-9f66-76446e61d988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7212.14076590538"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35de203-07f8-48f9-8ede-0a0bcba1ea27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 27ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, X_train_val)\n",
    "mase_predictions =  bagging_predict2(pred2, X_train_val)\n",
    "mape_predictions =  bagging_predict2(pred3, X_train_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d34d7f8-6cad-4fb9-b60b-9c4e03aba405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "original 0.23634 0.24183\n",
      "mase 0.23375 0.24019\n",
      "mape 1.18181 0.59899\n",
      "smape 0.24065 0.24574\n",
      "0.23626804\n",
      "0.24256758\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "\n",
    "smape_predictions_G = bagging_predict2(pred1, test_X)\n",
    "mase_predictions_G = bagging_predict2(pred2,test_X)\n",
    "mape_predictions_G = bagging_predict2(pred3,test_X)\n",
    "print('############################################################################################') \n",
    "print('############################################################################################') \n",
    "\n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G, mase_predictions_G,mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('original',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "pd.DataFrame(fin_pred_G.flatten()).to_csv('lstm.csv')\n",
    "\n",
    "\n",
    "concat_G = np.concatenate([mase_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mase',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mape',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('smape',mean_squared_error(test_y.flatten(),fin_pred_G.flatten()).round(5),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten()).round(5))\n",
    "\n",
    "print(np.mean([mean_squared_error(test_y.flatten(),mase_predictions_G[i].flatten()) for i in range(10)]))\n",
    "print(np.mean([mean_absolute_error(test_y.flatten(),mase_predictions_G[i].flatten()) for i in range(10)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
